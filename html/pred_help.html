<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.6.3" />
<title>pred_help API documentation</title>
<meta name="description" content="Python helper classes and functions to facilitate generation and display of predictions from CoreML, ONNX, and Torch models â€¦" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:1.1em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pred_help</code></h1>
</header>
<section id="section-intro">
<p>Python helper classes and functions to facilitate generation and display of predictions from CoreML, ONNX, and Torch models.</p>
<p>What's here:</p>
<p>class <strong><a title="pred_help.Classifier" href="#pred_help.Classifier"><code>Classifier</code></a></strong>
to invoke models, and collect and manage the resulting predictions.</p>
<p>class <strong><a title="pred_help.Results" href="#pred_help.Results"><code>Results</code></a></strong>
to browse and display results saved by Classifier</p>
<p>Model Execution and Calculation Functions:</p>
<ul>
<li><a title="pred_help.norm_for_imagenet" href="#pred_help.norm_for_imagenet"><code>norm_for_imagenet()</code></a> Normalize using ImageNet values for mean and std dev.</li>
<li><a title="pred_help.pred_for_coreml" href="#pred_help.pred_for_coreml"><code>pred_for_coreml()</code></a>
Classify an image using a native CoreML model.</li>
<li><a title="pred_help.pred_for_onnx" href="#pred_help.pred_for_onnx"><code>pred_for_onnx()</code></a>
Classify an image using a native ONNX model.</li>
<li><a title="pred_help.pred_for_o2c" href="#pred_help.pred_for_o2c"><code>pred_for_o2c()</code></a>
Classify an image using a CoreML model converted from ONNX.</li>
<li><a title="pred_help.softmax" href="#pred_help.softmax"><code>softmax()</code></a></li>
</ul>
<p>The general purpose of the <em>pred</em> functions is</p>
<ul>
<li>
<p>On input, take a standard image - e.g. RGB, pixels values from 0-255 - and transform it to be acceptable as input
to the specific model. This might require normalizing the data, or rescaling to the interval 0.0 - 1.0, etc.</p>
</li>
<li>
<p>On output, take the output from the model and transform it to an <a title="pred_help.ImagePrediction" href="#pred_help.ImagePrediction"><code>ImagePrediction</code></a></p>
</li>
</ul>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">&#34;&#34;&#34;
Python helper classes and functions to facilitate generation and display of predictions from CoreML, ONNX, and Torch models.

What&#39;s here:

class **`Classifier`**  to invoke models, and collect and manage the resulting predictions.

class **`Results`**  to browse and display results saved by Classifier

Model Execution and Calculation Functions:

- `norm_for_imagenet` Normalize using ImageNet values for mean and std dev.
- `pred_for_coreml`   Classify an image using a native CoreML model.
- `pred_for_onnx`     Classify an image using a native ONNX model.
- `pred_for_o2c`      Classify an image using a CoreML model converted from ONNX.
- `softmax`

The general purpose of the *pred* functions is

- On input, take a standard image - e.g. RGB, pixels values from 0-255 - and transform it to be acceptable as input
to the specific model. This might require normalizing the data, or rescaling to the interval 0.0 - 1.0, etc.

- On output, take the output from the model and transform it to an `ImagePrediction`

&#34;&#34;&#34;

# pdoc dictionary and helper function - used to document named tuples

__pdoc__ = {}
def _doc(key:str, val:str): __pdoc__[key] = val

# ----------------------------------------------------

from ms_util import *
from typing import Callable
from collections import namedtuple
import matplotlib
from matplotlib import pyplot as plt
from PIL import Image, ImageOps
import cv2

&#34;&#34;&#34; 
Formats and Data
&#34;&#34;&#34;

ImagePrediction = namedtuple(&#39;ImagePrediction&#39;, &#39;topI topP topL&#39;)
# doc
_doc(&#39;ImagePrediction&#39;,&#39;*Namedtuple*: standard format returned from *pred* functions&#39;)
_doc(&#39;ImagePrediction.topI&#39;, &#39;Indexes to top classes&#39;)
_doc(&#39;ImagePrediction.topP&#39;, &#39;Top probabilities&#39;)
_doc(&#39;ImagePrediction.topL&#39;, &#39;Top class Labels&#39;)

ImageRepo  = namedtuple(&#39;ImageRepo&#39; , &#39;mean std labels_url&#39;)
# doc
_doc(&#39;ImageRepo&#39;,&#39;*Namedtuple* that lists normalization stats and URLs for a repository&#39;)
_doc(&#39;ImageRepo.mean&#39;, &#39;*mean* values for normalization&#39;)
_doc(&#39;ImageRepo.std&#39;, &#39;*std* values for normalization&#39;)
_doc(&#39;ImageRepo.labels_url&#39;, &#39;URL for class labels&#39;)

PredParams = namedtuple(&#39;PredParams&#39;,&#39;func runtime imgsize labels&#39;)
# doc
_doc(&#39;PredParams&#39;, &#39;*Namedtuple*: specifies the prediction function, the runtime session for a model, the expected image size and the class labels&#39;)
_doc(&#39;PredParams.func&#39;, &#39;*pred* function to use&#39;)
_doc(&#39;PredParams.runtime&#39;, &#39;model object to invoke to generate predictions&#39;)
_doc(&#39;PredParams.imgsize&#39;, &#39;tuple for the expected image size&#39;)
_doc(&#39;PredParams.labels&#39;, &#39;List containing the class labels, or None&#39;)

### Data, Data Sources

imagenet = ImageRepo( mean   = [0.485, 0.456, 0.406], std= [0.229, 0.224, 0.225],
                     labels_url =&#39;https://s3.amazonaws.com/onnx-model-zoo/synset.txt&#39; )
&#34;&#34;&#34; Imagenet `ImageRepo` &#34;&#34;&#34;

cifar    = ImageRepo( mean = [0.491, 0.482, 0.447], std=[0.247, 0.243, 0.261], labels_url=None)
&#34;&#34;&#34; Cifar `ImageRepo` &#34;&#34;&#34;

mnist    = ImageRepo( mean = [0.15]*3, std  = [0.15]*3, labels_url=None)
&#34;&#34;&#34; Mnist `ImageRepo` &#34;&#34;&#34;


def if_None(x:any, default:any )-&gt;any:
  &#34;&#34;&#34;Return `default` if `x` is None.&#34;&#34;&#34;
  return x if x is not None else default

#  TODO: Move this to ms_util.py


def annotate_heatmap(im, data=None, valfmt=&#34;{x:.2f}&#34;, textcolors=[&#34;black&#34;, &#34;white&#34;],
                     threshold=None, **textkw):
  &#34;&#34;&#34;
  A function to annotate a heatmap.

  Args:
    im: The AxesImage to be labeled.
    data: Data used to annotate.  If None, the image&#39;s data is used.  Optional.
    valfmt: The format of the annotations inside the heatmap.
        This should either use the string format method, e.g. &#34;$ {x:.2f}&#34;,
        or be a `matplotlib.ticker.Formatter`.  Optional.
    textcolors: A list or array of two color specifications.  The first is used for
        values below a threshold, the second for those above.  Optional.
    threshold: Value in data units according to which the colors from textcolors are
        applied.  If None (the default) uses the middle of the colormap as
        separation.  Optional.
    **kwargs: All other arguments are forwarded to each call to `text` used to create
        the text labels.
  &#34;&#34;&#34;
  if not isinstance(data, (list, np.ndarray)): data = im.get_array()

  # Normalize the threshold to the images color range.
  threshold = im.norm(data.max()) / 2. if threshold is None else im.norm(threshold)

  # Set default alignment to center, but allow it to be overwritten by textkw.
  kw = dict(horizontalalignment=&#34;center&#34;,
            verticalalignment=&#34;center&#34;)
  kw.update(textkw)

  # Get the formatter in case a string is supplied

  if isinstance(valfmt, str):
    valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)

  # Loop over the data and create a `Text` for each &#34;pixel&#34;.
  # Change the text&#39;s color depending on the data.
  texts = []
  for i in range(data.shape[0]):
    for j in range(data.shape[1]):
      kw.update(color=textcolors[int(im.norm(data[i, j]) &gt; threshold)])
      text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)
      texts.append(text)

  return texts


def heatmap(data, row_labels, col_labels, ax=None,
            cbar_kw: dict = {}, cbarlabel=&#34;&#34;, **kwargs):
  &#34;&#34;&#34;
  Create a heatmap from a numpy array and two lists of labels.

  Args
  ----
  data
      A 2D numpy array of shape (N, M).
  row_labels
      A list or array of length N with the labels for the rows.
  col_labels
      A list or array of length M with the labels for the columns.
  ax
      A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If
      not provided, use current axes or create a new one.  Optional.
  cbar_kw
      A dictionary with arguments to `matplotlib.Figure.colorbar`.  Optional.
  cbarlabel
      The label for the colorbar.  Optional.
  **kwargs
      All other arguments are forwarded to `imshow`.
  &#34;&#34;&#34;

  if not ax: ax = plt.gca()

  # Plot the heatmap
  im = ax.imshow(data, **kwargs)

  # Create colorbar
  # cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)
  # cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=&#34;bottom&#34;)
  cbar = None

  # We want to show all ticks...
  ax.set_xticks(np.arange(data.shape[1]))
  ax.set_yticks(np.arange(data.shape[0]))

  # ... and label them with the respective list entries.
  ax.set_xticklabels(col_labels)
  ax.set_yticklabels(row_labels)

  # Let the horizontal axes labeling appear on top.
  ax.tick_params(top=True, bottom=False,
                 labeltop=True, labelbottom=False)

  # Rotate the tick labels and set their alignment.
  plt.setp(ax.get_xticklabels(), rotation=-30, ha=&#34;right&#34;,
           rotation_mode=&#34;anchor&#34;)

  # Turn spines off and create white grid.
  for edge, spine in ax.spines.items():
    spine.set_visible(False)

  ax.set_xticks(np.arange(data.shape[1] + 1) - .5, minor=True)
  ax.set_yticks(np.arange(data.shape[0] + 1) - .5, minor=True)
  ax.grid(which=&#34;minor&#34;, color=&#34;w&#34;, linestyle=&#39;-&#39;, linewidth=3)
  ax.tick_params(which=&#34;minor&#34;, bottom=False, left=False)

  return im, cbar


&#34;&#34;&#34; 
=== Layer Calculations ===============================
&#34;&#34;&#34;

def softmax(x: Uarray) -&gt; ndarray:
  &#34;&#34;&#34;
  Scale values to be between 0.0 - 1.0 so they can be used as probabilities.
  Formula is:

      exp(x)/sum(exp(x))

  Args:
    x (Union[List,ndarray]): Values on which to calculate the softmax.
                             Should be ndarray or convertible to an ndarray

  Returns:
    softmax as ndarray

  &#34;&#34;&#34;

  np_exp = np.exp(np.array(x))
  return np_exp / np.sum(np_exp, axis=0)


def norm_for_imagenet(img: Uimage) -&gt; ndarray:
  &#34;&#34;&#34;
  Normalize an image using ImageNet values for mean and standard deviation.

  Args:
    img(ndarray,Image.Image): Image data with values between 0-255.
      If not an ndarray, must be convertible to one.
      Shape must be either (3,_,_) or (_,_,3)

  Return:
    Normalized image data as an ndarray[float32]

  Raises:
    ValueError: If image shape is not (3,_,_) or (_,_,3), or number of dimensions is not 3

  Notes:
    For each pixel in each channel, scale to the interval [0.0, 1.0] and then
    normalize using the mean and standard deviation from ImageNet.
    The input values are assumed to range from 0-255,
    input type is assumed to be an ndarray,
    or an image format that can be converted to an ndarray.
    Here is the formula:

        normalized_value = (value/255.0 - mean)/stddev

        mean = [0.485, 0.456, 0.406]
        std  = [0.229, 0.224, 0.225]
  &#34;&#34;&#34;
  img = np.array(img)
  if img.ndim != 3: raise ValueError(f&#34;Image has {img.ndim} dimensions, expected 3&#34;)

  # Mean and Stddev for image net
  mean = imagenet.mean
  std = imagenet.std

  shape = img.shape
  nimg = np.zeros(shape).astype(&#39;float32&#39;)

  # for each pixel in each channel, divide the value by 255 to get value between [0, 1] and then normalize
  if shape[0] == 3:
    for i in range(3): nimg[i, :, :] = (img[i, :, :] / 255.0 - mean[i]) / std[i]
  elif shape[2] == 3:
    for i in range(3): nimg[:, :, i] = (img[:, :, i] / 255.0 - mean[i]) / std[i]
  else:
    raise ValueError(f&#34;Image shape is {shape}, expected (3,_,_) or (_,_,3)&#34;)

  return nimg

&#34;&#34;&#34; 
Model Execution and ImagePrediction
&#34;&#34;&#34;

def _image_pred(topI=Uarray, topP=Uarray, topL=Uarray)-&gt;ImagePrediction:
  &#34;&#34;&#34; Construct and return an `ImagePrediction` tuple&#34;&#34;&#34;
  return ImagePrediction(topI=topI, topP=np.array(topP), topL=topL)

_no_results = ([0], [0.00], [&#34;No Results&#34;])
&#34;&#34;&#34; Default ImagePrediction values&#34;&#34;&#34;


&#34;&#34;&#34;
=== Prediction Functions ===============================
&#34;&#34;&#34;

def pred_for_coreml(model:Callable, img:Uimage, labels=None, n_top:int=3 )-&gt;ImagePrediction:
  &#34;&#34;&#34;
  Run a native CoreML Classifier and return the top results as a standardized *ImagePrediction*.
  If you want to run a CoreML model **converted** from ONNX, use `pred_for_o2c`

  Args:
    model (object): The coreml model to use for the prediction
    img (Image.Image): Fitted image to use for test
    n_top (int): Number of top values to return (default 3)
    labels (list): Not needed for CoreML, ignored. Kept as an argument for consistency with other &#34;pred&#34; functions.

  Returns:
    ImagePrediction

  Notes:
    The the description for the native CoreML Resnet50 model states that it takes images in BGR format.
    However, converting input images from RGB to BGR results in much poorer predictions than leaving them in RGB.
    So I&#39;m assuming that the model does some pre-processing to check the image and do the conversion on its own.
    Or maybe the description is incorrect.
  &#34;&#34;&#34;

  topI, topP, topL = _no_results
  in_name, out_name = None, None

  try:

    description = model.get_spec().description
    in_name   = description.input[0].name
    out_name  = description.output[0].name

    y       = model.predict({in_name:img}, useCPUOnly=True)

    pdict   = y[out_name]
    prob    = [v for v in pdict.values()]
    labels  = [k for k in pdict.keys()]
    topI    = np.argsort(prob)[:-(n_top+1):-1]
    topP    = np.array([prob[i]  for i in topI])
    topL    = [labels[i] for i in topI]

  except Exception as e :
    print()
    print(f&#34;Exception from pred_for_coreml(input={in_name}, output={out_name})&#34;)
    print(e)

  return _image_pred(topI=topI, topP=topP, topL=topL)


def pred_for_o2c(model, img:Uimage,  labels=None, n_top:int=3 )-&gt;ImagePrediction:
  &#34;&#34;&#34;
  Run a CoreML Classifier model that was converted from ONNX;
  return the top results as a standardized *ImagePrediction*.

  This function converts the output from the final layer to a list of probabilities,
  then extracts the top items and associated labels. This step is needed because
  the ONNX Resnet50 model does not contain a final softmax layer, and the
  conversion to CoreML does not add one. (The native CoreML Resnet50 does have a softmax layer)

  Args:
    model (object): The CoreML model to use for inference
    img (Image.Image): The image to process. Expected to be an image with values 0-255
    n_top (int): Number of top values to return (default 3)
    labels ([str]): Class Labels for output, if needed

  Return:
    ImagePrediction
  &#34;&#34;&#34;

  topI, topP, topL = _no_results
  in_name,  out_name = None, None

  try:

    description = model.get_spec().description
    in_name   = description.input[0].name
    out_name  = description.output[0].name

    y         = model.predict({in_name:img}, useCPUOnly=True)
    y_out     = y[out_name]
    out_type  = type(y_out)

    if out_type is ndarray: # Case 1: conversion from onnx-&gt;coreml
      pvals = np.squeeze(y_out)
    elif out_type is dict:  # Case 2: conversion from torch-&gt;onnx-&gt;coreml
      pvals   = np.array([v for v in y_out.values()])
      labels  = np.array([k for k in y_out.keys()])
    else:                   # Case ?: Don&#39;t know ... probably an error
      raise TypeError(f&#34;Type {out_type} of model output is unexpected or incorrect&#34;)

    prob    = softmax(pvals)
    topI    = np.argsort(prob)[:-(n_top+1):-1]
    topP    = [ prob[i]   for i in topI ]
    topL    = [ &#39;None&#39; if labels is None else labels[i] for i in topI ]

  except Exception as e:
    print(f&#34;Exception from pred_for_o2c(input={in_name}, output={out_name})&#34;)
    print(e)

  pred = _image_pred(topI=topI, topP=topP, topL=topL)
  return pred


def pred_for_onnx(sess:object, img:Uimage, labels=None, n_top=3 )-&gt;ImagePrediction:
  &#34;&#34;&#34;
  Run the ONNX Classifier model and return the top results as a standardized *ImagePrediction*.

  This function

    - normalizes the image data,
    - if needed, massages the data to a shape of (3,_,_)
    - runs the model using `onnxruntime`
    - converts the output from the final layer to a list of probabilities,
    - extracts the top items and associated labels.

  Args:
    sess (object): The ONNX run-time session(model) to use for prediction
    img (Union[ndarray,Image.Image]):  Image or image data to use for test
    n_top (int): Number of top values to return
    labels ([str]): Class labels for output

  Return:
    ImagePrediction
  &#34;&#34;&#34;
  # Use the image to generate acceptable input for the model
  # - move axes if needed, normalize, add a dimension to make it (1,3,224,224)

  topI, topP, topL = _no_results

  # Get input and output names for the model
  input0 = sess.get_inputs()[0]
  output = sess.get_outputs()[0]
  input0_name = input0.name
  output_name = output.name

  try:
    np_img    = np.array(img)
    rs_img    = np.moveaxis(np_img,[0,1,2],[1,2,0]) if np_img.shape[2] == 3 else np_img
    norm_img  = norm_for_imagenet(rs_img)
    x         = np.array([norm_img])

  # Run the model
    r = sess.run([output_name], {input0_name: x})

    # Get predictions from the results
    res  = np.squeeze(np.array(r))  # eliminate dimensions w/ len=1 , e.g. from (1,1,1000) --&gt; (1000,)
    prob = softmax(res)
    topI = np.argsort(prob)[:-(n_top+1):-1]
    topP = [ prob[i]    for i in topI ]
    topL = [ labels[i]  for i in topI ]

  except Exception as e:
    print()
    print(f&#34;Exception from pred_for_onnx(input={input0_name}, output={output_name})&#34;)
    print(e)

  return _image_pred(topI=topI, topP=topP, topL=topL)


def pred_for_torch(model:Callable, img:Uimage, labels=None, n_top:int=3, )-&gt;ImagePrediction:
  &#34;&#34;&#34;
  Run the Torch Classifier model return the top results.

  This function converts the output from the final layer to a list of probabilities,
  then extracts the top items and associated labels. This step is needed because the
  Torch Resnet50 model does not contain a final softmax layer

  Args:
    model (object): The CoreML model to use for inference
    img (Uimage): The image to classify. Either an Image or image data in a ndarray with values 0-255.
    labels ([str]): Class Labels for output
    n_top (int): Number of top values to return (default 3)


  Return:
    ImagePrediction

  &#34;&#34;&#34;
  import torch
  from torch.autograd import Variable
  from torchvision.transforms.functional import to_tensor
  from torchvision.transforms.functional import normalize
  from torch.nn.functional import softmax as t_softmax

  topI, topP, topL = _no_results

  try:
    norm_img      = normalize(to_tensor(img), mean=imagenet.mean, std=imagenet.std)
    reshaped_img  = norm_img.reshape(tuple([1]) + tuple(norm_img.shape))
    img_tensor    = torch.as_tensor(reshaped_img, dtype=torch.float)
    x = Variable(img_tensor)

    y = model(x)

    tout      = t_softmax(y, dim=1)
    top       = tout.topk(n_top)
    topI = top.indices[0].tolist()
    topP = top.values[0].tolist()
    topL = [ labels[i]   for i in topI ]

  except Exception as e :
    print()
    print(f&#34;Exception from pred_for_torch(input={&#39;&#39;}, output={&#39;&#39;})&#34;)
    print(e)

  return _image_pred(topI=topI, topP=topP, topL=topL)




def _fmt_imagenet_label( label: str) -&gt; str:
  &#34;&#34;&#34;Reverse the order of id and name, so that name comes first&#34;&#34;&#34;
  import re
  if re.search(&#34;n\d+ &#34;, label):
    t1, t2 = re.split(&#39; &#39;, label, maxsplit=1)
    t1 = f&#34;({t1})&#34;
  else:
    t1, t2 = &#39;&#39;, label
  return f&#34;{t2:16.24s} {t1}&#34;


def _fmt_results(pred: ImagePrediction, n2show=2) -&gt; str:
  &#34;&#34;&#34;
  Return a formatted string for all results in the ImagePrediction tuple

  Args:
    pred(ImagePrediction): &#34;ImagePrediction&#34; named tuple

  Returns:
    a formatted string for result at index &#39;idx&#39;

  Example:
    &#39;46.05%  Eagle &#39;
  &#34;&#34;&#34;
  results = &#39;&#39;
  for i in range(n2show):
    l = _fmt_imagenet_label(pred.topL[i])
    p = pred.topP[i]
    results += f&#34;  {p:06.02%} {l}\n&#34;
  return results


def show_pred(img_path:Upath, pred:ImagePrediction, model_id=&#34;Model&#34;,
              pred2show=3, figsize=(2.0, 3.5), img_size=(200, 200),
              fontsize=12, fontfamily=&#39;monospace&#39;):
  &#34;&#34;&#34;
  Display the image and predictions side by side.

  Args:
    img_path (Union[str,Path]): The path to the image
    pred (ImagePrediction): The prediction tuple returned from the *pred* function
    model_id (str): The model short name
    pred2show (ing): How many of the top probabilities to display
    figsize (tuple): Size of the subplot
    img_size (tuple): Size of the image
    fontsize (int): Font size
    fontfamily (str): Font family
  &#34;&#34;&#34;

  def add_text(x,y,txt):
    ax.text(x, y, txt, verticalalignment=&#39;top&#39;, fontsize=fontsize, fontfamily=fontfamily)

  img_path  = Path(img_path)
  indent    = 20
  y_start   = 4
  y_per_line= int(1.9 * fontsize) + 2

  # Show the image  without frame or ticks
  _, ax = plt.subplots(1, 1, figsize=figsize, subplot_kw=dict(frame_on=False, xticks=[], yticks=[]))
  ax.imshow( ImageOps.fit(Image.open(img_path), size=img_size, method=Image.NEAREST) )

  # Show the image file name
  x = img_size[0] + indent
  y = y_start
  add_text(x, y, img_path.name)

  # Show the model abbr.
  x += indent
  y += y_per_line
  add_text(x, y, model_id)

  # Show the prediction probabilities
  y += y_per_line
  add_text(x, y, _fmt_results(pred, n2show=pred2show))

  plt.show()

  #

# def preds_result(img_path: Upath, pred_dict: dict) -&gt; dict:
#   &#34;&#34;&#34; Get all predictions for one image and return them in result item dict&#34;&#34;&#34;
#
#   # Fix RGBA images (or others) on the fly, if possible ...
#   img = Image.open(img_path)
#   if img.mode != &#39;RGB&#39;:
#     print(f&#39;Converting {img.name} to RGB from {img.mode} &#39;)
#     img = img.convert(&#39;RGB&#39;)
#
#   # Some of the models are picky about the image size ...
#   img_sized = { 224 : ImageOps.fit(img, (224, 224), method=_resize_method, centering=(0.5, 0.4) ),
#                 300 : ImageOps.fit(img, (300, 300), method=_resize_method, centering=(0.5, 0.4) ) }
#
#   # Create result item for this image using the prediction dictionary
#   res = { &#39;name&#39;: img.name, &#39;path&#39;:img_path}
#   for model, pred in pred_dict:
#     res[model] = pred.func(pred.runtime, img_sized[pred.imgsize], pred.labels)
#
#   return res
  
class Classifier:
  &#34;&#34;&#34;
  This class keeps the models to be run and captures their predictions.
  &#34;&#34;&#34;
  def __init__(self, params:dict, top_count=3, num_images=8, resize_method=Image.NEAREST):
    &#34;&#34;&#34;
    Args:
      params (dict): A dictionary containing a `PredParams` for each model.
        Specifies the *pred* function,  arguments to use to invoke each model.
      top_count (int): How many top prediction values (class indexes, probabilities) to keep
      num_images (int): Placeholder for number of images to process.
      resize_method (enum): How to resize the image. Defaults to Image.NEAREST
    &#34;&#34;&#34;
    self.pred_params  = params
    self.model_list = [m for m in self.pred_params.keys() ]
    self.model_dict = {m:i for i,m in enumerate(self.model_list)}
    self.num_models = len(self.model_list)
    self.num_imgs   = num_images
    self.resize_method = resize_method
    self.top_count  = top_count
    self.top_probs  = None
    self.top_classes= None
    self.results    = None
    self.stat_int   = None
        
        
  def i2m(self,i:int)-&gt;str:
    &#34;&#34;&#34;
    Return the model short name for the index `i`
    Args:
      i (int): Index into the `model_list`
    &#34;&#34;&#34;
    return self.model_list[i]


  def m2i(self,m:str)-&gt;int:
    &#34;&#34;&#34;
    Return the index into the model_list for the model short name
    Args:
      m (str): Short name (or id, or abbreviation) for the model.
    &#34;&#34;&#34;
    return self.model_dict[m]
  
        
  def classify(self, imgs:list, top_count=None)-&gt;list:
    &#34;&#34;&#34;
    Generate predictions for each of the images, by model.
    Populates the Classifier `results` list, the `top_probs` ndarray, and the `top_classes` ndarray.
    
    Args:
      imgs (list): List of image file paths.
        Updates the value of `num_images` in the Classifier
      top_count (int): Reset how many predictions to keep for each img and model.
        If None, use the value already set in the Classifier.
        If set, updates the value saved in the Classifier
        
    Returns:
        The `results` list. There is one entry in the list for each image. Each entry
        is a dict with predictions for the image, by model.
    &#34;&#34;&#34;
    # Generate values, allocate arrays
    self.num_imgs    = len(imgs)
    self.stat_int    = max(16,int(self.num_imgs/4))
    self.top_probs   = np.empty((self.num_models, self.num_imgs, self.top_count), dtype=float)
    self.top_classes = np.empty((self.num_models, self.num_imgs, self.top_count), dtype=int)
    self.results     = [self.pred_params] * self.num_imgs
    if top_count is not None: self.top_count = top_count
    
    # Get predictions for each image, save results
    # Save copies of class indexes and probabilities for later calculation
    
    for i, img_path in enumerate(imgs):
        self.results[i] = self.preds_for(img_path)
        
        for im, model in enumerate(self.model_list): 
            model_result = self.results[i][model]
            self.top_classes[im][i] = np.array(model_result.topI)
            self.top_probs[im][i]   = np.array(model_result.topP)
            
        if (i%self.stat_int) == 0 : 
            print(f&#34;{i} of {self.num_imgs} processed, most recent is {img_path.name}&#34;)

    print(f&#34;Total of {self.num_imgs} images processed&#34;)
    return self.results

  def preds_for(self, img_path: Upath) -&gt; dict:
    &#34;&#34;&#34;
     Get all predictions for one image and return them in result item dict.
     Will attempt to convert non-RGB images to RGB.

    Args:
      img_path (Upath): Path to the image

    Uses:
      The `pred_params` values from the Classifier
      
    Returns:
      A `returns` list item. A dict with image name, path, and
      the predicted `top_count` classes, probabilities and labels
      for each of the models.

    &#34;&#34;&#34;
    # Open image, fix RGBA images (or others) on the fly, if possible ...
    img_path = Path(img_path)
    img = Image.open(img_path)
    if img.mode != &#39;RGB&#39;:
      print(f&#39;Converting {img_path.name} to RGB from {img.mode} &#39;)
    img = img.convert(&#39;RGB&#39;)
    
    # Some of the models are picky about the image size ...
    mid_top  = (0.5, 0.4)
    resize  = self.resize_method
    img_sized= {
        224 : ImageOps.fit(img,(224,224), centering=mid_top, method=resize, ),
        300 : ImageOps.fit(img,(300,300), centering=mid_top, method=resize, ),
    }
    # Create result item for this image using the prediction dictionary
    topn = self.top_count
    result_item = { &#39;name&#39;: img_path.name, &#39;path&#39;:img_path }
    for mname, pred_params in self.pred_params.items():
        pred_for = pred_params.func
        model  = pred_params.runtime
        img    = img_sized[pred_params.imgsize]
        labels = pred_params.labels
        result_item[mname] = pred_for(model, img, labels, topn )

    return result_item


class Results:
  &#34;&#34;&#34;
  Methods and parameters to
  - display the results of classifying a list of images
  - compare results
  - calculate and display agreement between models
  &#34;&#34;&#34;
  def __init__(self, classifier:Classifier, pred2show=2, figsize=(3.0,3.5),
                     cols=1, imgsize=(224,224), fontsize=12, fontfamily=&#39;monospace&#39;):
    &#34;&#34;&#34;

    Args:
      classifier (Classifier): The Classifier object containing the results.
      pred2show (int): How many predictions to display
      fontsize (int): The fontsize
      fontfamily (str): The font family

    Returns:
      Results Object

    &#34;&#34;&#34;
    super()
    self.classifier = classifier
    self.resize_method = classifier.resize_method
    self.model_list = classifier.model_list
    self.model_dict = classifier.model_dict
    self.results    = classifier.results
    self.results_len = len(self.results)
    self.num_imgs    = classifier.num_imgs
    self.top_classes = classifier.top_classes
    self.top_probs  = classifier.top_probs
    self.fontsize   = fontsize
    self.fontfamily = fontfamily
    self.figsize    = figsize
    self.imgsize    = imgsize
    self.cols       = cols
    self.pred2show  = pred2show
    self.m2i = classifier.m2i
    self.i2m = classifier.i2m
    self.x   = 0
    self.y   = 0
    #
    self.ax           = None
    self.model_id     = None
    self.agree        = None
    self.agree_counts = None
    self.agree_diff   = None
    #
    self._init_agreement()



  def _init_agreement(self):

    cf  = self.classifier
    tc  = cf.top_classes
    tp  = cf.top_probs
    CML = cf.m2i(&#39;cml&#39;)

    # Allocate the 2 and 3 dim arrays we will need
    nm  = cf.num_models
    ni  = cf.num_imgs
    self.agree        = np.empty((nm, nm, ni), dtype=bool)
    self.agree_counts = np.empty((nm, nm), dtype=int)
    self.agree_diff   = np.empty((nm, nm, ni), dtype=float)

    # Populate the agreement tensors (CML will need to be revised ... see below)
    for im, m in enumerate(cf.model_list):
      for ik, k in enumerate(cf.model_list):
        self.agree[im, ik]        = tc[im, :, 0] == tc[ik, :, 0]
        self.agree_counts[im, ik] = self.agree[im, ik].sum()

    # Get accurate CML agreement counts
    for ir, r in enumerate(self.results):
      cml_comp = self._cml_compare(r)
      self.agree[CML, :, ir] = cml_comp
      self.agree[:, CML, ir] = cml_comp

    # Replace CML in the `agree` and `agree_counts` with accurate results
    for im, m in enumerate(cf.model_list):
      cml_sum = self.agree[CML, im, :].sum()
      self.agree_counts[CML, im] = cml_sum
      self.agree_counts[im, CML] = cml_sum

    # Populate the agreement difference matrix
    # If the two models agree on the top class, use the difference in probabilities,
    # if not, use 1.0 (they disagree 100% = no agreement )
    for im, m in enumerate(cf.model_list):
      for ik, k in enumerate(cf.model_list):
        for ir, r in enumerate(self.results):
          self.agree_diff[im, ik, ir]   = abs(tp[im, ir , 0] - tp[ik, ir , 0 ]) if self.agree[im,ik,ir] else 1.0


  def _cml_compare(self, res_item: dict) -&gt; ndarray:
    &#34;&#34;&#34;
    A results comparison function just for cml ...

    For this result item (i.e. image), compare the top CML label to the top label for the other models
    Return boolean array indicating which models agree with CML and which do not
    &#34;&#34;&#34;
    # Allocate an empty array, get the CML label, clean it up
    cml_agree = np.empty(len(self.model_list), dtype=bool)
    cml_label = res_item[&#39;cml&#39;].topL[0]
    #print(&#34;&#34;)
    #print(f&#34;cml label   = {cml_label}&#34;)
    cml_label.strip(&#39; ,-:;&#39;)
    #print(f&#34;cml label s = {cml_label}&#34;)
    # Compare the CML label to each of the top labels for the other models
    for im, m in enumerate(self.model_list):
      topL0 = res_item[m].topL[0]
      mitem = re.search(cml_label, topL0)
      cml_agree[im] = (mitem is not None)
      #print(f&#34;m     = {m}&#34;)
      #print(f&#34;topL0 = {topL0}&#34;)
      #print(f&#34;mitem = {mitem}&#34;)
      #print(f&#34;im    = {im}&#34;)
      #print(f&#34;cml_agree[im] = { cml_agree[im] }&#34;)
      #print(&#34;&#34;)
    #
    #print(f&#34;cml_agree = {cml_agree}&#34;)
    return cml_agree

  def agree_matrix(self):
    &#34;&#34;&#34;Show a heat-mapped agreement matrix&#34;&#34;&#34;
    fig, ax = plt.subplots(figsize=(8, 8))
    am, _   = heatmap(self.agree_counts, self.model_list, self.model_list,
                      ax=ax, cmap=&#34;PiYG&#34;, cbarlabel=&#34;Agreement&#34;)
    annotate_heatmap(am, valfmt=&#34;{x:d}&#34;, textcolors=[&#34;white&#34;, &#34;black&#34;], size=12)
    am = ax.imshow(self.agree_counts)
    return am

  def best_worst( self, model1:str, model2:str )-&gt;(int,int):
    &#34;&#34;&#34;
    **Agreement** - Returns indexes to the results with the best(= min diff) and worst(= max diff)
    agreement between two models

    Args:
      model1 (str): model id specified in model_params( e.g. &#34;onnx&#34;)
      model2 (str): model id specified in in model_params

    &#34;&#34;&#34;
    M1, M2 = self.m2i(model1), self.m2i(model2)
    # Copy the result array bkz we are going to zap it
    mmd = self.agree_diff[M1,M2].copy()
    best = mmd.argmin()
    # Zero all the &#34;1.0&#34; that represent the diff for non-matching classes
    mmd[mmd == 1.0] = 0.0
    # Now we can get an argmax diff for those classes that do match
    worst  = mmd.argmax()
    return best, worst


  def most_least(self)-&gt;dict:
    &#34;&#34;&#34;**Certainty** - Return the most and least certain results for all models&#34;&#34;&#34;
    tp = self.classifier.top_probs
    model_most_least  = [ [tp[im,:,0].argmax(), tp[im,:,0].argmin()] for im,m in enumerate(self.classifier.model_list) ]
    return model_most_least


  def _add_pred(self, pred:ImagePrediction, model_id:str=None, n2show=2,
                x:int=None, y:int=None):
    &#34;&#34;&#34;
    Add a Prediction to an existing axes.

    Args:
      pred (ImagePrediction): Image Prediction named tuple
      model_id (str): Model short name
      n2show (int): How many predictions to display
      x (int): starting x position for the text
      y (int): starting y position for the text

    Returns:
      Current value of x,y coordinates (x,y is also saved in Results object)
    &#34;&#34;&#34;

    ax = self.ax
    x  = if_None(x,self.x)
    y  = if_None(y,self.y)
    model  = if_None(model_id, self.model_id)
    fontsize = self.fontsize

    name_indent      = 10
    y_per_line  = int(1.9 * fontsize)+2
    y_between   = fontsize // 3
    results_indent  = name_indent + int(4* fontsize)

    # Show the model short name
    y +=  y_between
    ax.text(x + name_indent, y, model, verticalalignment=&#39;top&#39;,
            fontsize=self.fontsize, fontfamily=self.fontfamily)

    # Show the prediction results
    ax.text(x + results_indent, y, _fmt_results(pred, n2show=n2show),
             verticalalignment=&#39;top&#39;, fontsize=self.fontsize, fontfamily=self.fontfamily)
    self.x = x
    self.y = y + n2show * y_per_line
    return x, y

  def show_agreement(self,model1:str):
    &#34;&#34;&#34;Show agreement counts between `model1` and the others&#34;&#34;&#34;
    cf    = self.classifier
    M1    = cf.m2i(model1)
    nimgs = cf.num_imgs
    for im, m in enumerate(cf.model_list):
      agreed = self.agree_counts[M1, im]
      print(f&#34;{model1:7} and {m:7} agree on {agreed:4} of {nimgs:4} or {agreed / nimgs:2.2%}&#34;)


  def show_one(self, result:dict, models:list=None,
               pred2show:int = None, img_size=None, figsize=None, fontsize=None, fontfamily=None):
    &#34;&#34;&#34;
    Show selected or all predictions for one image ( = one result list item )

    Args:
      result (dict): The predictions for each model.
      models (list): For display, overrides the list of model names kept in Classifier.
      pred2show (int): How many of the top results to show for each prediction.

    Returns:
      ax : Object from plt.subplot call.
      None if there are no predictions

    &#34;&#34;&#34;
    cf          = self.classifier
    models      = if_None(models,   cf.model_list)
    figsize     = if_None(figsize,  self.figsize)
    img_size    = if_None(img_size, self.imgsize)
    fontsize    = if_None(fontsize, self.fontsize)
    fontfamily  = if_None(fontfamily, self.fontfamily)
    pred2show   = if_None(pred2show, self.pred2show)

    img_path    = Path(result[&#39;path&#39;])

    y_start = 0
    y_per_line = int(1.9 * fontsize)+2
    indent = 20

    # Show the image without frame or ticks
    fimg = ImageOps.fit(Image.open(img_path), size=img_size, method=cf.resize_method, centering=(0.5, 0.4))
    fig, ax = plt.subplots(1, 1, figsize=figsize, subplot_kw=dict(frame_on=False, xticks=[], yticks=[]))
    self.ax = ax
    ax.imshow(fimg)

    # Show the image file name
    x = img_size[0] + indent - 4
    y = y_start
    ax.text(x, y, img_path.name, fontsize=fontsize, fontfamily=fontfamily, verticalalignment=&#39;top&#39;)

    # Show the model(s) and their prediction probabilities
    self.x = x + indent
    self.y = y + y_per_line
    for m in models:
      self._add_pred(result[m], model_id=m, n2show=pred2show)

    plt.show()
    return ax

  def show(self, items:Union[int,list,tuple], models=None ):
    &#34;&#34;&#34;
    Show items from the result list
    Args:
      items (list): List of indexes into the results list.
        Or an int to show one item only.
      models (list): Constrains which models to show results for.

    &#34;&#34;&#34;
    results = self.classifier.results
    rlen    = self.results_len

    if type(items) is int:
      if items &lt;= rlen :
        self.show_one(results[items], models=models)
      return

    if type(items) is not list and type(items) is not tuple:
      raise TypeError(f&#34;type(items)={type(items)}; &#39;items&#39; must be an int or a list of ints&#34;)

    for n in items :
      if n &lt;= rlen :
        self.show_one(results[n], models=models)

  # def show_result(self, result:dict, pred2show:int=3, figsize=(3.0,3.5),
  #                 img_size=(224,224), fontsize=12, fontfamily=&#39;monospaced&#39;) :
  #   &#34;&#34;&#34;
  #   Show selected or all predictions for one image ( = one result item )
  #     Args:
  #       result(dict): The path to the image.
  #       img(Image): Image to use (optional, if not passed, image is read from &#39;img_path&#39;)
  #       pred2show(int): How many of the top results to show for each prediction.
  #         Set to True to display immediately after show_pred call
  #         Set to False to allow additional &#34;_add_pred&#34; calls before displaying.
  #         Use `plt.show()` to display when complete
  #
  #     Returns:
  #        axs: object from plt.subplot call
  #        x:   x position
  #        y:   y position
  #        None if there are no predictions
  #   &#34;&#34;&#34;
  #
  #   models = [m for m in result.keys() if m not in [&#39;name&#39;,&#39;path&#39;]]
  #
  #   img_path = Path(result[&#39;path&#39;])
  #
  #   y_start       = 4
  #   y_per_line    = int(1.9 * self.fontsize)
  #   indent        = 20
  #
  #   # Show the image  without frame or ticks
  #   fimg     = ImageOps.fit(Image.open(img_path), size=img_size, method=Image.NEAREST, centering=(0.5, 0.4))
  #   fig, ax  = plt.subplots(1, 1, figsize=figsize, subplot_kw=dict(frame_on=False, xticks=[], yticks=[]))
  #   self.ax  = ax
  #   ax.imshow(fimg)
  #
  #   # Show the image file name
  #   x = img_size[0] + indent - 4
  #   y = y_start
  #   ax.text(x, y, img_path.name, fontsize=fontsize, fontfamily=fontsize)
  #
  #   # Show the model(s) and their prediction probabilities
  #   self.x = x + indent
  #   self.y = y + y_per_line + 2
  #   for m in models[1:len(models)]:
  #      self._add_pred( result[m], model_id=m, n2show=pred2show)
  #
  #   plt.show()
  #   return ax

  def show_random(self,count=5,models=None) :
    import random
    display_list = random.sample(range(self.results_len), count)
    display_list.sort()
    print(f&#34;\nShowing results {display_list} \n  and top {self.pred2show} probabilities for each model&#34;)
    self.show(display_list, models=models)</code></pre>
</details>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-variables">Global variables</h2>
<dl>
<dt id="pred_help.cifar"><code class="name">var <span class="ident">cifar</span></code></dt>
<dd>
<section class="desc"><p>Cifar <a title="pred_help.ImageRepo" href="#pred_help.ImageRepo"><code>ImageRepo</code></a></p></section>
</dd>
<dt id="pred_help.imagenet"><code class="name">var <span class="ident">imagenet</span></code></dt>
<dd>
<section class="desc"><p>Imagenet <a title="pred_help.ImageRepo" href="#pred_help.ImageRepo"><code>ImageRepo</code></a></p></section>
</dd>
<dt id="pred_help.mnist"><code class="name">var <span class="ident">mnist</span></code></dt>
<dd>
<section class="desc"><p>Mnist <a title="pred_help.ImageRepo" href="#pred_help.ImageRepo"><code>ImageRepo</code></a></p></section>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="pred_help.annotate_heatmap"><code class="name flex">
<span>def <span class="ident">annotate_heatmap</span></span>(<span>im, data=None, valfmt='{x:.2f}', textcolors=['black', 'white'], threshold=None, **textkw)</span>
</code></dt>
<dd>
<section class="desc"><p>A function to annotate a heatmap.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>im</code></strong></dt>
<dd>The AxesImage to be labeled.</dd>
<dt><strong><code>data</code></strong></dt>
<dd>Data used to annotate.
If None, the image's data is used.
Optional.</dd>
<dt><strong><code>valfmt</code></strong></dt>
<dd>The format of the annotations inside the heatmap.
This should either use the string format method, e.g. "$ {x:.2f}",
or be a <code>matplotlib.ticker.Formatter</code>.
Optional.</dd>
<dt><strong><code>textcolors</code></strong></dt>
<dd>A list or array of two color specifications.
The first is used for
values below a threshold, the second for those above.
Optional.</dd>
<dt><strong><code>threshold</code></strong></dt>
<dd>Value in data units according to which the colors from textcolors are
applied.
If None (the default) uses the middle of the colormap as
separation.
Optional.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>All other arguments are forwarded to each call to <code>text</code> used to create
the text labels.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def annotate_heatmap(im, data=None, valfmt=&#34;{x:.2f}&#34;, textcolors=[&#34;black&#34;, &#34;white&#34;],
                     threshold=None, **textkw):
  &#34;&#34;&#34;
  A function to annotate a heatmap.

  Args:
    im: The AxesImage to be labeled.
    data: Data used to annotate.  If None, the image&#39;s data is used.  Optional.
    valfmt: The format of the annotations inside the heatmap.
        This should either use the string format method, e.g. &#34;$ {x:.2f}&#34;,
        or be a `matplotlib.ticker.Formatter`.  Optional.
    textcolors: A list or array of two color specifications.  The first is used for
        values below a threshold, the second for those above.  Optional.
    threshold: Value in data units according to which the colors from textcolors are
        applied.  If None (the default) uses the middle of the colormap as
        separation.  Optional.
    **kwargs: All other arguments are forwarded to each call to `text` used to create
        the text labels.
  &#34;&#34;&#34;
  if not isinstance(data, (list, np.ndarray)): data = im.get_array()

  # Normalize the threshold to the images color range.
  threshold = im.norm(data.max()) / 2. if threshold is None else im.norm(threshold)

  # Set default alignment to center, but allow it to be overwritten by textkw.
  kw = dict(horizontalalignment=&#34;center&#34;,
            verticalalignment=&#34;center&#34;)
  kw.update(textkw)

  # Get the formatter in case a string is supplied

  if isinstance(valfmt, str):
    valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)

  # Loop over the data and create a `Text` for each &#34;pixel&#34;.
  # Change the text&#39;s color depending on the data.
  texts = []
  for i in range(data.shape[0]):
    for j in range(data.shape[1]):
      kw.update(color=textcolors[int(im.norm(data[i, j]) &gt; threshold)])
      text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)
      texts.append(text)

  return texts</code></pre>
</details>
</dd>
<dt id="pred_help.heatmap"><code class="name flex">
<span>def <span class="ident">heatmap</span></span>(<span>data, row_labels, col_labels, ax=None, cbar_kw={}, cbarlabel='', **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Create a heatmap from a numpy array and two lists of labels.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>A 2D numpy array of shape (N, M).</dd>
<dt><strong><code>row_labels</code></strong></dt>
<dd>A list or array of length N with the labels for the rows.</dd>
<dt><strong><code>col_labels</code></strong></dt>
<dd>A list or array of length M with the labels for the columns.</dd>
<dt><strong><code>ax</code></strong></dt>
<dd>A <code>matplotlib.axes.Axes</code> instance to which the heatmap is plotted.
If
not provided, use current axes or create a new one.
Optional.</dd>
<dt><strong><code>cbar_kw</code></strong></dt>
<dd>A dictionary with arguments to <code>matplotlib.Figure.colorbar</code>.
Optional.</dd>
<dt><strong><code>cbarlabel</code></strong></dt>
<dd>The label for the colorbar.
Optional.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>All other arguments are forwarded to <code>imshow</code>.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def heatmap(data, row_labels, col_labels, ax=None,
            cbar_kw: dict = {}, cbarlabel=&#34;&#34;, **kwargs):
  &#34;&#34;&#34;
  Create a heatmap from a numpy array and two lists of labels.

  Args
  ----
  data
      A 2D numpy array of shape (N, M).
  row_labels
      A list or array of length N with the labels for the rows.
  col_labels
      A list or array of length M with the labels for the columns.
  ax
      A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If
      not provided, use current axes or create a new one.  Optional.
  cbar_kw
      A dictionary with arguments to `matplotlib.Figure.colorbar`.  Optional.
  cbarlabel
      The label for the colorbar.  Optional.
  **kwargs
      All other arguments are forwarded to `imshow`.
  &#34;&#34;&#34;

  if not ax: ax = plt.gca()

  # Plot the heatmap
  im = ax.imshow(data, **kwargs)

  # Create colorbar
  # cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)
  # cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=&#34;bottom&#34;)
  cbar = None

  # We want to show all ticks...
  ax.set_xticks(np.arange(data.shape[1]))
  ax.set_yticks(np.arange(data.shape[0]))

  # ... and label them with the respective list entries.
  ax.set_xticklabels(col_labels)
  ax.set_yticklabels(row_labels)

  # Let the horizontal axes labeling appear on top.
  ax.tick_params(top=True, bottom=False,
                 labeltop=True, labelbottom=False)

  # Rotate the tick labels and set their alignment.
  plt.setp(ax.get_xticklabels(), rotation=-30, ha=&#34;right&#34;,
           rotation_mode=&#34;anchor&#34;)

  # Turn spines off and create white grid.
  for edge, spine in ax.spines.items():
    spine.set_visible(False)

  ax.set_xticks(np.arange(data.shape[1] + 1) - .5, minor=True)
  ax.set_yticks(np.arange(data.shape[0] + 1) - .5, minor=True)
  ax.grid(which=&#34;minor&#34;, color=&#34;w&#34;, linestyle=&#39;-&#39;, linewidth=3)
  ax.tick_params(which=&#34;minor&#34;, bottom=False, left=False)

  return im, cbar</code></pre>
</details>
</dd>
<dt id="pred_help.if_None"><code class="name flex">
<span>def <span class="ident">if_None</span></span>(<span>x, default)</span>
</code></dt>
<dd>
<section class="desc"><p>Return <code>default</code> if <code>x</code> is None.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def if_None(x:any, default:any )-&gt;any:
  &#34;&#34;&#34;Return `default` if `x` is None.&#34;&#34;&#34;
  return x if x is not None else default</code></pre>
</details>
</dd>
<dt id="pred_help.norm_for_imagenet"><code class="name flex">
<span>def <span class="ident">norm_for_imagenet</span></span>(<span>img)</span>
</code></dt>
<dd>
<section class="desc"><p>Normalize an image using ImageNet values for mean and standard deviation.</p>
<h2 id="args">Args</h2>
<p>img(ndarray,Image.Image): Image data with values between 0-255.
If not an ndarray, must be convertible to one.
Shape must be either (3,<em>,</em>) or (<em>,</em>,3)</p>
<h2 id="return">Return</h2>
<p>Normalized image data as an ndarray[float32]</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><strong><code>ValueError</code></strong></dt>
<dd>If image shape is not (3,<em>,</em>) or (<em>,</em>,3), or number of dimensions is not 3</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>For each pixel in each channel, scale to the interval [0.0, 1.0] and then
normalize using the mean and standard deviation from ImageNet.
The input values are assumed to range from 0-255,
input type is assumed to be an ndarray,
or an image format that can be converted to an ndarray.
Here is the formula:</p>
<pre><code>normalized_value = (value/255.0 - mean)/stddev

mean = [0.485, 0.456, 0.406]
std  = [0.229, 0.224, 0.225]
</code></pre></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def norm_for_imagenet(img: Uimage) -&gt; ndarray:
  &#34;&#34;&#34;
  Normalize an image using ImageNet values for mean and standard deviation.

  Args:
    img(ndarray,Image.Image): Image data with values between 0-255.
      If not an ndarray, must be convertible to one.
      Shape must be either (3,_,_) or (_,_,3)

  Return:
    Normalized image data as an ndarray[float32]

  Raises:
    ValueError: If image shape is not (3,_,_) or (_,_,3), or number of dimensions is not 3

  Notes:
    For each pixel in each channel, scale to the interval [0.0, 1.0] and then
    normalize using the mean and standard deviation from ImageNet.
    The input values are assumed to range from 0-255,
    input type is assumed to be an ndarray,
    or an image format that can be converted to an ndarray.
    Here is the formula:

        normalized_value = (value/255.0 - mean)/stddev

        mean = [0.485, 0.456, 0.406]
        std  = [0.229, 0.224, 0.225]
  &#34;&#34;&#34;
  img = np.array(img)
  if img.ndim != 3: raise ValueError(f&#34;Image has {img.ndim} dimensions, expected 3&#34;)

  # Mean and Stddev for image net
  mean = imagenet.mean
  std = imagenet.std

  shape = img.shape
  nimg = np.zeros(shape).astype(&#39;float32&#39;)

  # for each pixel in each channel, divide the value by 255 to get value between [0, 1] and then normalize
  if shape[0] == 3:
    for i in range(3): nimg[i, :, :] = (img[i, :, :] / 255.0 - mean[i]) / std[i]
  elif shape[2] == 3:
    for i in range(3): nimg[:, :, i] = (img[:, :, i] / 255.0 - mean[i]) / std[i]
  else:
    raise ValueError(f&#34;Image shape is {shape}, expected (3,_,_) or (_,_,3)&#34;)

  return nimg</code></pre>
</details>
</dd>
<dt id="pred_help.pred_for_coreml"><code class="name flex">
<span>def <span class="ident">pred_for_coreml</span></span>(<span>model, img, labels=None, n_top=3)</span>
</code></dt>
<dd>
<section class="desc"><p>Run a native CoreML Classifier and return the top results as a standardized <em>ImagePrediction</em>.
If you want to run a CoreML model <strong>converted</strong> from ONNX, use <a title="pred_help.pred_for_o2c" href="#pred_help.pred_for_o2c"><code>pred_for_o2c()</code></a></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>object</code></dt>
<dd>The coreml model to use for the prediction</dd>
<dt><strong><code>img</code></strong> :&ensp;<code>Image.Image</code></dt>
<dd>Fitted image to use for test</dd>
<dt><strong><code>n_top</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of top values to return (default 3)</dd>
<dt><strong><code>labels</code></strong> :&ensp;<code>list</code></dt>
<dd>Not needed for CoreML, ignored. Kept as an argument for consistency with other "pred" functions.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><a title="pred_help.ImagePrediction" href="#pred_help.ImagePrediction"><code>ImagePrediction</code></a></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The the description for the native CoreML Resnet50 model states that it takes images in BGR format.
However, converting input images from RGB to BGR results in much poorer predictions than leaving them in RGB.
So I'm assuming that the model does some pre-processing to check the image and do the conversion on its own.
Or maybe the description is incorrect.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def pred_for_coreml(model:Callable, img:Uimage, labels=None, n_top:int=3 )-&gt;ImagePrediction:
  &#34;&#34;&#34;
  Run a native CoreML Classifier and return the top results as a standardized *ImagePrediction*.
  If you want to run a CoreML model **converted** from ONNX, use `pred_for_o2c`

  Args:
    model (object): The coreml model to use for the prediction
    img (Image.Image): Fitted image to use for test
    n_top (int): Number of top values to return (default 3)
    labels (list): Not needed for CoreML, ignored. Kept as an argument for consistency with other &#34;pred&#34; functions.

  Returns:
    ImagePrediction

  Notes:
    The the description for the native CoreML Resnet50 model states that it takes images in BGR format.
    However, converting input images from RGB to BGR results in much poorer predictions than leaving them in RGB.
    So I&#39;m assuming that the model does some pre-processing to check the image and do the conversion on its own.
    Or maybe the description is incorrect.
  &#34;&#34;&#34;

  topI, topP, topL = _no_results
  in_name, out_name = None, None

  try:

    description = model.get_spec().description
    in_name   = description.input[0].name
    out_name  = description.output[0].name

    y       = model.predict({in_name:img}, useCPUOnly=True)

    pdict   = y[out_name]
    prob    = [v for v in pdict.values()]
    labels  = [k for k in pdict.keys()]
    topI    = np.argsort(prob)[:-(n_top+1):-1]
    topP    = np.array([prob[i]  for i in topI])
    topL    = [labels[i] for i in topI]

  except Exception as e :
    print()
    print(f&#34;Exception from pred_for_coreml(input={in_name}, output={out_name})&#34;)
    print(e)

  return _image_pred(topI=topI, topP=topP, topL=topL)</code></pre>
</details>
</dd>
<dt id="pred_help.pred_for_o2c"><code class="name flex">
<span>def <span class="ident">pred_for_o2c</span></span>(<span>model, img, labels=None, n_top=3)</span>
</code></dt>
<dd>
<section class="desc"><p>Run a CoreML Classifier model that was converted from ONNX;
return the top results as a standardized <em>ImagePrediction</em>.</p>
<p>This function converts the output from the final layer to a list of probabilities,
then extracts the top items and associated labels. This step is needed because
the ONNX Resnet50 model does not contain a final softmax layer, and the
conversion to CoreML does not add one. (The native CoreML Resnet50 does have a softmax layer)</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>object</code></dt>
<dd>The CoreML model to use for inference</dd>
<dt><strong><code>img</code></strong> :&ensp;<code>Image.Image</code></dt>
<dd>The image to process. Expected to be an image with values 0-255</dd>
<dt><strong><code>n_top</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of top values to return (default 3)</dd>
<dt><strong><code>labels</code></strong> :&ensp;[<code>str</code>]</dt>
<dd>Class Labels for output, if needed</dd>
</dl>
<h2 id="return">Return</h2>
<dl>
<dt><strong><a title="pred_help.ImagePrediction" href="#pred_help.ImagePrediction"><code>ImagePrediction</code></a></strong></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def pred_for_o2c(model, img:Uimage,  labels=None, n_top:int=3 )-&gt;ImagePrediction:
  &#34;&#34;&#34;
  Run a CoreML Classifier model that was converted from ONNX;
  return the top results as a standardized *ImagePrediction*.

  This function converts the output from the final layer to a list of probabilities,
  then extracts the top items and associated labels. This step is needed because
  the ONNX Resnet50 model does not contain a final softmax layer, and the
  conversion to CoreML does not add one. (The native CoreML Resnet50 does have a softmax layer)

  Args:
    model (object): The CoreML model to use for inference
    img (Image.Image): The image to process. Expected to be an image with values 0-255
    n_top (int): Number of top values to return (default 3)
    labels ([str]): Class Labels for output, if needed

  Return:
    ImagePrediction
  &#34;&#34;&#34;

  topI, topP, topL = _no_results
  in_name,  out_name = None, None

  try:

    description = model.get_spec().description
    in_name   = description.input[0].name
    out_name  = description.output[0].name

    y         = model.predict({in_name:img}, useCPUOnly=True)
    y_out     = y[out_name]
    out_type  = type(y_out)

    if out_type is ndarray: # Case 1: conversion from onnx-&gt;coreml
      pvals = np.squeeze(y_out)
    elif out_type is dict:  # Case 2: conversion from torch-&gt;onnx-&gt;coreml
      pvals   = np.array([v for v in y_out.values()])
      labels  = np.array([k for k in y_out.keys()])
    else:                   # Case ?: Don&#39;t know ... probably an error
      raise TypeError(f&#34;Type {out_type} of model output is unexpected or incorrect&#34;)

    prob    = softmax(pvals)
    topI    = np.argsort(prob)[:-(n_top+1):-1]
    topP    = [ prob[i]   for i in topI ]
    topL    = [ &#39;None&#39; if labels is None else labels[i] for i in topI ]

  except Exception as e:
    print(f&#34;Exception from pred_for_o2c(input={in_name}, output={out_name})&#34;)
    print(e)

  pred = _image_pred(topI=topI, topP=topP, topL=topL)
  return pred</code></pre>
</details>
</dd>
<dt id="pred_help.pred_for_onnx"><code class="name flex">
<span>def <span class="ident">pred_for_onnx</span></span>(<span>sess, img, labels=None, n_top=3)</span>
</code></dt>
<dd>
<section class="desc"><p>Run the ONNX Classifier model and return the top results as a standardized <em>ImagePrediction</em>.</p>
<p>This function</p>
<ul>
<li>normalizes the image data,</li>
<li>if needed, massages the data to a shape of (3,<em>,</em>)</li>
<li>runs the model using <code>onnxruntime</code></li>
<li>converts the output from the final layer to a list of probabilities,</li>
<li>extracts the top items and associated labels.</li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sess</code></strong> :&ensp;<code>object</code></dt>
<dd>The ONNX run-time session(model) to use for prediction</dd>
<dt><strong><code>img</code></strong> :&ensp;<code>Union</code>[<code>ndarray</code>,<code>Image.Image</code>]</dt>
<dd>Image or image data to use for test</dd>
<dt><strong><code>n_top</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of top values to return</dd>
<dt><strong><code>labels</code></strong> :&ensp;[<code>str</code>]</dt>
<dd>Class labels for output</dd>
</dl>
<h2 id="return">Return</h2>
<dl>
<dt><strong><a title="pred_help.ImagePrediction" href="#pred_help.ImagePrediction"><code>ImagePrediction</code></a></strong></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def pred_for_onnx(sess:object, img:Uimage, labels=None, n_top=3 )-&gt;ImagePrediction:
  &#34;&#34;&#34;
  Run the ONNX Classifier model and return the top results as a standardized *ImagePrediction*.

  This function

    - normalizes the image data,
    - if needed, massages the data to a shape of (3,_,_)
    - runs the model using `onnxruntime`
    - converts the output from the final layer to a list of probabilities,
    - extracts the top items and associated labels.

  Args:
    sess (object): The ONNX run-time session(model) to use for prediction
    img (Union[ndarray,Image.Image]):  Image or image data to use for test
    n_top (int): Number of top values to return
    labels ([str]): Class labels for output

  Return:
    ImagePrediction
  &#34;&#34;&#34;
  # Use the image to generate acceptable input for the model
  # - move axes if needed, normalize, add a dimension to make it (1,3,224,224)

  topI, topP, topL = _no_results

  # Get input and output names for the model
  input0 = sess.get_inputs()[0]
  output = sess.get_outputs()[0]
  input0_name = input0.name
  output_name = output.name

  try:
    np_img    = np.array(img)
    rs_img    = np.moveaxis(np_img,[0,1,2],[1,2,0]) if np_img.shape[2] == 3 else np_img
    norm_img  = norm_for_imagenet(rs_img)
    x         = np.array([norm_img])

  # Run the model
    r = sess.run([output_name], {input0_name: x})

    # Get predictions from the results
    res  = np.squeeze(np.array(r))  # eliminate dimensions w/ len=1 , e.g. from (1,1,1000) --&gt; (1000,)
    prob = softmax(res)
    topI = np.argsort(prob)[:-(n_top+1):-1]
    topP = [ prob[i]    for i in topI ]
    topL = [ labels[i]  for i in topI ]

  except Exception as e:
    print()
    print(f&#34;Exception from pred_for_onnx(input={input0_name}, output={output_name})&#34;)
    print(e)

  return _image_pred(topI=topI, topP=topP, topL=topL)</code></pre>
</details>
</dd>
<dt id="pred_help.pred_for_torch"><code class="name flex">
<span>def <span class="ident">pred_for_torch</span></span>(<span>model, img, labels=None, n_top=3)</span>
</code></dt>
<dd>
<section class="desc"><p>Run the Torch Classifier model return the top results.</p>
<p>This function converts the output from the final layer to a list of probabilities,
then extracts the top items and associated labels. This step is needed because the
Torch Resnet50 model does not contain a final softmax layer</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>object</code></dt>
<dd>The CoreML model to use for inference</dd>
<dt><strong><code>img</code></strong> :&ensp;<code>Uimage</code></dt>
<dd>The image to classify. Either an Image or image data in a ndarray with values 0-255.</dd>
<dt><strong><code>labels</code></strong> :&ensp;[<code>str</code>]</dt>
<dd>Class Labels for output</dd>
<dt><strong><code>n_top</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of top values to return (default 3)</dd>
</dl>
<h2 id="return">Return</h2>
<dl>
<dt><strong><a title="pred_help.ImagePrediction" href="#pred_help.ImagePrediction"><code>ImagePrediction</code></a></strong></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def pred_for_torch(model:Callable, img:Uimage, labels=None, n_top:int=3, )-&gt;ImagePrediction:
  &#34;&#34;&#34;
  Run the Torch Classifier model return the top results.

  This function converts the output from the final layer to a list of probabilities,
  then extracts the top items and associated labels. This step is needed because the
  Torch Resnet50 model does not contain a final softmax layer

  Args:
    model (object): The CoreML model to use for inference
    img (Uimage): The image to classify. Either an Image or image data in a ndarray with values 0-255.
    labels ([str]): Class Labels for output
    n_top (int): Number of top values to return (default 3)


  Return:
    ImagePrediction

  &#34;&#34;&#34;
  import torch
  from torch.autograd import Variable
  from torchvision.transforms.functional import to_tensor
  from torchvision.transforms.functional import normalize
  from torch.nn.functional import softmax as t_softmax

  topI, topP, topL = _no_results

  try:
    norm_img      = normalize(to_tensor(img), mean=imagenet.mean, std=imagenet.std)
    reshaped_img  = norm_img.reshape(tuple([1]) + tuple(norm_img.shape))
    img_tensor    = torch.as_tensor(reshaped_img, dtype=torch.float)
    x = Variable(img_tensor)

    y = model(x)

    tout      = t_softmax(y, dim=1)
    top       = tout.topk(n_top)
    topI = top.indices[0].tolist()
    topP = top.values[0].tolist()
    topL = [ labels[i]   for i in topI ]

  except Exception as e :
    print()
    print(f&#34;Exception from pred_for_torch(input={&#39;&#39;}, output={&#39;&#39;})&#34;)
    print(e)

  return _image_pred(topI=topI, topP=topP, topL=topL)</code></pre>
</details>
</dd>
<dt id="pred_help.show_pred"><code class="name flex">
<span>def <span class="ident">show_pred</span></span>(<span>img_path, pred, model_id='Model', pred2show=3, figsize=(2.0, 3.5), img_size=(200, 200), fontsize=12, fontfamily='monospace')</span>
</code></dt>
<dd>
<section class="desc"><p>Display the image and predictions side by side.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>img_path</code></strong> :&ensp;<code>Union</code>[<code>str</code>,<code>Path</code>]</dt>
<dd>The path to the image</dd>
<dt><strong><code>pred</code></strong> :&ensp;<a title="pred_help.ImagePrediction" href="#pred_help.ImagePrediction"><code>ImagePrediction</code></a></dt>
<dd>The prediction tuple returned from the <em>pred</em> function</dd>
<dt><strong><code>model_id</code></strong> :&ensp;<code>str</code></dt>
<dd>The model short name</dd>
<dt><strong><code>pred2show</code></strong> :&ensp;<code>ing</code></dt>
<dd>How many of the top probabilities to display</dd>
<dt><strong><code>figsize</code></strong> :&ensp;<code>tuple</code></dt>
<dd>Size of the subplot</dd>
<dt><strong><code>img_size</code></strong> :&ensp;<code>tuple</code></dt>
<dd>Size of the image</dd>
<dt><strong><code>fontsize</code></strong> :&ensp;<code>int</code></dt>
<dd>Font size</dd>
<dt><strong><code>fontfamily</code></strong> :&ensp;<code>str</code></dt>
<dd>Font family</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def show_pred(img_path:Upath, pred:ImagePrediction, model_id=&#34;Model&#34;,
              pred2show=3, figsize=(2.0, 3.5), img_size=(200, 200),
              fontsize=12, fontfamily=&#39;monospace&#39;):
  &#34;&#34;&#34;
  Display the image and predictions side by side.

  Args:
    img_path (Union[str,Path]): The path to the image
    pred (ImagePrediction): The prediction tuple returned from the *pred* function
    model_id (str): The model short name
    pred2show (ing): How many of the top probabilities to display
    figsize (tuple): Size of the subplot
    img_size (tuple): Size of the image
    fontsize (int): Font size
    fontfamily (str): Font family
  &#34;&#34;&#34;

  def add_text(x,y,txt):
    ax.text(x, y, txt, verticalalignment=&#39;top&#39;, fontsize=fontsize, fontfamily=fontfamily)

  img_path  = Path(img_path)
  indent    = 20
  y_start   = 4
  y_per_line= int(1.9 * fontsize) + 2

  # Show the image  without frame or ticks
  _, ax = plt.subplots(1, 1, figsize=figsize, subplot_kw=dict(frame_on=False, xticks=[], yticks=[]))
  ax.imshow( ImageOps.fit(Image.open(img_path), size=img_size, method=Image.NEAREST) )

  # Show the image file name
  x = img_size[0] + indent
  y = y_start
  add_text(x, y, img_path.name)

  # Show the model abbr.
  x += indent
  y += y_per_line
  add_text(x, y, model_id)

  # Show the prediction probabilities
  y += y_per_line
  add_text(x, y, _fmt_results(pred, n2show=pred2show))

  plt.show()</code></pre>
</details>
</dd>
<dt id="pred_help.softmax"><code class="name flex">
<span>def <span class="ident">softmax</span></span>(<span>x)</span>
</code></dt>
<dd>
<section class="desc"><p>Scale values to be between 0.0 - 1.0 so they can be used as probabilities.
Formula is:</p>
<pre><code>exp(x)/sum(exp(x))
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>Union</code>[<code>List</code>,<code>ndarray</code>]</dt>
<dd>Values on which to calculate the softmax.
Should be ndarray or convertible to an ndarray</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><a title="pred_help.softmax" href="#pred_help.softmax"><code>softmax()</code></a> <code>as</code> <code>ndarray</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def softmax(x: Uarray) -&gt; ndarray:
  &#34;&#34;&#34;
  Scale values to be between 0.0 - 1.0 so they can be used as probabilities.
  Formula is:

      exp(x)/sum(exp(x))

  Args:
    x (Union[List,ndarray]): Values on which to calculate the softmax.
                             Should be ndarray or convertible to an ndarray

  Returns:
    softmax as ndarray

  &#34;&#34;&#34;

  np_exp = np.exp(np.array(x))
  return np_exp / np.sum(np_exp, axis=0)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="pred_help.Classifier"><code class="flex name class">
<span>class <span class="ident">Classifier</span></span>
<span>(</span><span>params, top_count=3, num_images=8, resize_method=0)</span>
</code></dt>
<dd>
<section class="desc"><p>This class keeps the models to be run and captures their predictions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>dict</code></dt>
<dd>A dictionary containing a <a title="pred_help.PredParams" href="#pred_help.PredParams"><code>PredParams</code></a> for each model.
Specifies the <em>pred</em> function,
arguments to use to invoke each model.</dd>
<dt><strong><code>top_count</code></strong> :&ensp;<code>int</code></dt>
<dd>How many top prediction values (class indexes, probabilities) to keep</dd>
<dt><strong><code>num_images</code></strong> :&ensp;<code>int</code></dt>
<dd>Placeholder for number of images to process.</dd>
<dt><strong><code>resize_method</code></strong> :&ensp;<code>enum</code></dt>
<dd>How to resize the image. Defaults to Image.NEAREST</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class Classifier:
  &#34;&#34;&#34;
  This class keeps the models to be run and captures their predictions.
  &#34;&#34;&#34;
  def __init__(self, params:dict, top_count=3, num_images=8, resize_method=Image.NEAREST):
    &#34;&#34;&#34;
    Args:
      params (dict): A dictionary containing a `PredParams` for each model.
        Specifies the *pred* function,  arguments to use to invoke each model.
      top_count (int): How many top prediction values (class indexes, probabilities) to keep
      num_images (int): Placeholder for number of images to process.
      resize_method (enum): How to resize the image. Defaults to Image.NEAREST
    &#34;&#34;&#34;
    self.pred_params  = params
    self.model_list = [m for m in self.pred_params.keys() ]
    self.model_dict = {m:i for i,m in enumerate(self.model_list)}
    self.num_models = len(self.model_list)
    self.num_imgs   = num_images
    self.resize_method = resize_method
    self.top_count  = top_count
    self.top_probs  = None
    self.top_classes= None
    self.results    = None
    self.stat_int   = None
        
        
  def i2m(self,i:int)-&gt;str:
    &#34;&#34;&#34;
    Return the model short name for the index `i`
    Args:
      i (int): Index into the `model_list`
    &#34;&#34;&#34;
    return self.model_list[i]


  def m2i(self,m:str)-&gt;int:
    &#34;&#34;&#34;
    Return the index into the model_list for the model short name
    Args:
      m (str): Short name (or id, or abbreviation) for the model.
    &#34;&#34;&#34;
    return self.model_dict[m]
  
        
  def classify(self, imgs:list, top_count=None)-&gt;list:
    &#34;&#34;&#34;
    Generate predictions for each of the images, by model.
    Populates the Classifier `results` list, the `top_probs` ndarray, and the `top_classes` ndarray.
    
    Args:
      imgs (list): List of image file paths.
        Updates the value of `num_images` in the Classifier
      top_count (int): Reset how many predictions to keep for each img and model.
        If None, use the value already set in the Classifier.
        If set, updates the value saved in the Classifier
        
    Returns:
        The `results` list. There is one entry in the list for each image. Each entry
        is a dict with predictions for the image, by model.
    &#34;&#34;&#34;
    # Generate values, allocate arrays
    self.num_imgs    = len(imgs)
    self.stat_int    = max(16,int(self.num_imgs/4))
    self.top_probs   = np.empty((self.num_models, self.num_imgs, self.top_count), dtype=float)
    self.top_classes = np.empty((self.num_models, self.num_imgs, self.top_count), dtype=int)
    self.results     = [self.pred_params] * self.num_imgs
    if top_count is not None: self.top_count = top_count
    
    # Get predictions for each image, save results
    # Save copies of class indexes and probabilities for later calculation
    
    for i, img_path in enumerate(imgs):
        self.results[i] = self.preds_for(img_path)
        
        for im, model in enumerate(self.model_list): 
            model_result = self.results[i][model]
            self.top_classes[im][i] = np.array(model_result.topI)
            self.top_probs[im][i]   = np.array(model_result.topP)
            
        if (i%self.stat_int) == 0 : 
            print(f&#34;{i} of {self.num_imgs} processed, most recent is {img_path.name}&#34;)

    print(f&#34;Total of {self.num_imgs} images processed&#34;)
    return self.results

  def preds_for(self, img_path: Upath) -&gt; dict:
    &#34;&#34;&#34;
     Get all predictions for one image and return them in result item dict.
     Will attempt to convert non-RGB images to RGB.

    Args:
      img_path (Upath): Path to the image

    Uses:
      The `pred_params` values from the Classifier
      
    Returns:
      A `returns` list item. A dict with image name, path, and
      the predicted `top_count` classes, probabilities and labels
      for each of the models.

    &#34;&#34;&#34;
    # Open image, fix RGBA images (or others) on the fly, if possible ...
    img_path = Path(img_path)
    img = Image.open(img_path)
    if img.mode != &#39;RGB&#39;:
      print(f&#39;Converting {img_path.name} to RGB from {img.mode} &#39;)
    img = img.convert(&#39;RGB&#39;)
    
    # Some of the models are picky about the image size ...
    mid_top  = (0.5, 0.4)
    resize  = self.resize_method
    img_sized= {
        224 : ImageOps.fit(img,(224,224), centering=mid_top, method=resize, ),
        300 : ImageOps.fit(img,(300,300), centering=mid_top, method=resize, ),
    }
    # Create result item for this image using the prediction dictionary
    topn = self.top_count
    result_item = { &#39;name&#39;: img_path.name, &#39;path&#39;:img_path }
    for mname, pred_params in self.pred_params.items():
        pred_for = pred_params.func
        model  = pred_params.runtime
        img    = img_sized[pred_params.imgsize]
        labels = pred_params.labels
        result_item[mname] = pred_for(model, img, labels, topn )

    return result_item</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="pred_help.Classifier.classify"><code class="name flex">
<span>def <span class="ident">classify</span></span>(<span>self, imgs, top_count=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Generate predictions for each of the images, by model.
Populates the Classifier <code>results</code> list, the <code>top_probs</code> ndarray, and the <code>top_classes</code> ndarray.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>imgs</code></strong> :&ensp;<code>list</code></dt>
<dd>List of image file paths.
Updates the value of <code>num_images</code> in the Classifier</dd>
<dt><strong><code>top_count</code></strong> :&ensp;<code>int</code></dt>
<dd>Reset how many predictions to keep for each img and model.
If None, use the value already set in the Classifier.
If set, updates the value saved in the Classifier</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>The <code>results</code> list. There is one entry in the list for each image. Each entry
is a dict with predictions for the image, by model.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def classify(self, imgs:list, top_count=None)-&gt;list:
  &#34;&#34;&#34;
  Generate predictions for each of the images, by model.
  Populates the Classifier `results` list, the `top_probs` ndarray, and the `top_classes` ndarray.
  
  Args:
    imgs (list): List of image file paths.
      Updates the value of `num_images` in the Classifier
    top_count (int): Reset how many predictions to keep for each img and model.
      If None, use the value already set in the Classifier.
      If set, updates the value saved in the Classifier
      
  Returns:
      The `results` list. There is one entry in the list for each image. Each entry
      is a dict with predictions for the image, by model.
  &#34;&#34;&#34;
  # Generate values, allocate arrays
  self.num_imgs    = len(imgs)
  self.stat_int    = max(16,int(self.num_imgs/4))
  self.top_probs   = np.empty((self.num_models, self.num_imgs, self.top_count), dtype=float)
  self.top_classes = np.empty((self.num_models, self.num_imgs, self.top_count), dtype=int)
  self.results     = [self.pred_params] * self.num_imgs
  if top_count is not None: self.top_count = top_count
  
  # Get predictions for each image, save results
  # Save copies of class indexes and probabilities for later calculation
  
  for i, img_path in enumerate(imgs):
      self.results[i] = self.preds_for(img_path)
      
      for im, model in enumerate(self.model_list): 
          model_result = self.results[i][model]
          self.top_classes[im][i] = np.array(model_result.topI)
          self.top_probs[im][i]   = np.array(model_result.topP)
          
      if (i%self.stat_int) == 0 : 
          print(f&#34;{i} of {self.num_imgs} processed, most recent is {img_path.name}&#34;)

  print(f&#34;Total of {self.num_imgs} images processed&#34;)
  return self.results</code></pre>
</details>
</dd>
<dt id="pred_help.Classifier.i2m"><code class="name flex">
<span>def <span class="ident">i2m</span></span>(<span>self, i)</span>
</code></dt>
<dd>
<section class="desc"><p>Return the model short name for the index <code>i</code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>i</code></strong> :&ensp;<code>int</code></dt>
<dd>Index into the <code>model_list</code></dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def i2m(self,i:int)-&gt;str:
  &#34;&#34;&#34;
  Return the model short name for the index `i`
  Args:
    i (int): Index into the `model_list`
  &#34;&#34;&#34;
  return self.model_list[i]</code></pre>
</details>
</dd>
<dt id="pred_help.Classifier.m2i"><code class="name flex">
<span>def <span class="ident">m2i</span></span>(<span>self, m)</span>
</code></dt>
<dd>
<section class="desc"><p>Return the index into the model_list for the model short name</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>m</code></strong> :&ensp;<code>str</code></dt>
<dd>Short name (or id, or abbreviation) for the model.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def m2i(self,m:str)-&gt;int:
  &#34;&#34;&#34;
  Return the index into the model_list for the model short name
  Args:
    m (str): Short name (or id, or abbreviation) for the model.
  &#34;&#34;&#34;
  return self.model_dict[m]</code></pre>
</details>
</dd>
<dt id="pred_help.Classifier.preds_for"><code class="name flex">
<span>def <span class="ident">preds_for</span></span>(<span>self, img_path)</span>
</code></dt>
<dd>
<section class="desc"><p>Get all predictions for one image and return them in result item dict.
Will attempt to convert non-RGB images to RGB.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>img_path</code></strong> :&ensp;<code>Upath</code></dt>
<dd>Path to the image</dd>
</dl>
<h2 id="uses">Uses</h2>
<p>The <code>pred_params</code> values from the Classifier</p>
<h2 id="returns">Returns</h2>
<p>A <code>returns</code> list item. A dict with image name, path, and
the predicted <code>top_count</code> classes, probabilities and labels
for each of the models.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def preds_for(self, img_path: Upath) -&gt; dict:
  &#34;&#34;&#34;
   Get all predictions for one image and return them in result item dict.
   Will attempt to convert non-RGB images to RGB.

  Args:
    img_path (Upath): Path to the image

  Uses:
    The `pred_params` values from the Classifier
    
  Returns:
    A `returns` list item. A dict with image name, path, and
    the predicted `top_count` classes, probabilities and labels
    for each of the models.

  &#34;&#34;&#34;
  # Open image, fix RGBA images (or others) on the fly, if possible ...
  img_path = Path(img_path)
  img = Image.open(img_path)
  if img.mode != &#39;RGB&#39;:
    print(f&#39;Converting {img_path.name} to RGB from {img.mode} &#39;)
  img = img.convert(&#39;RGB&#39;)
  
  # Some of the models are picky about the image size ...
  mid_top  = (0.5, 0.4)
  resize  = self.resize_method
  img_sized= {
      224 : ImageOps.fit(img,(224,224), centering=mid_top, method=resize, ),
      300 : ImageOps.fit(img,(300,300), centering=mid_top, method=resize, ),
  }
  # Create result item for this image using the prediction dictionary
  topn = self.top_count
  result_item = { &#39;name&#39;: img_path.name, &#39;path&#39;:img_path }
  for mname, pred_params in self.pred_params.items():
      pred_for = pred_params.func
      model  = pred_params.runtime
      img    = img_sized[pred_params.imgsize]
      labels = pred_params.labels
      result_item[mname] = pred_for(model, img, labels, topn )

  return result_item</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pred_help.ImagePrediction"><code class="flex name class">
<span>class <span class="ident">ImagePrediction</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p><em>Namedtuple</em>: standard format returned from <em>pred</em> functions</p></section>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.tuple</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="pred_help.ImagePrediction.topI"><code class="name">var <span class="ident">topI</span></code></dt>
<dd>
<section class="desc"><p>Indexes to top classes</p></section>
</dd>
<dt id="pred_help.ImagePrediction.topL"><code class="name">var <span class="ident">topL</span></code></dt>
<dd>
<section class="desc"><p>Top class Labels</p></section>
</dd>
<dt id="pred_help.ImagePrediction.topP"><code class="name">var <span class="ident">topP</span></code></dt>
<dd>
<section class="desc"><p>Top probabilities</p></section>
</dd>
</dl>
</dd>
<dt id="pred_help.ImageRepo"><code class="flex name class">
<span>class <span class="ident">ImageRepo</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p><em>Namedtuple</em> that lists normalization stats and URLs for a repository</p></section>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.tuple</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="pred_help.ImageRepo.labels_url"><code class="name">var <span class="ident">labels_url</span></code></dt>
<dd>
<section class="desc"><p>URL for class labels</p></section>
</dd>
<dt id="pred_help.ImageRepo.mean"><code class="name">var <span class="ident">mean</span></code></dt>
<dd>
<section class="desc"><p><em>mean</em> values for normalization</p></section>
</dd>
<dt id="pred_help.ImageRepo.std"><code class="name">var <span class="ident">std</span></code></dt>
<dd>
<section class="desc"><p><em>std</em> values for normalization</p></section>
</dd>
</dl>
</dd>
<dt id="pred_help.PredParams"><code class="flex name class">
<span>class <span class="ident">PredParams</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p><em>Namedtuple</em>: specifies the prediction function, the runtime session for a model, the expected image size and the class labels</p></section>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.tuple</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="pred_help.PredParams.func"><code class="name">var <span class="ident">func</span></code></dt>
<dd>
<section class="desc"><p><em>pred</em> function to use</p></section>
</dd>
<dt id="pred_help.PredParams.imgsize"><code class="name">var <span class="ident">imgsize</span></code></dt>
<dd>
<section class="desc"><p>tuple for the expected image size</p></section>
</dd>
<dt id="pred_help.PredParams.labels"><code class="name">var <span class="ident">labels</span></code></dt>
<dd>
<section class="desc"><p>List containing the class labels, or None</p></section>
</dd>
<dt id="pred_help.PredParams.runtime"><code class="name">var <span class="ident">runtime</span></code></dt>
<dd>
<section class="desc"><p>model object to invoke to generate predictions</p></section>
</dd>
</dl>
</dd>
<dt id="pred_help.Results"><code class="flex name class">
<span>class <span class="ident">Results</span></span>
<span>(</span><span>classifier, pred2show=2, figsize=(3.0, 3.5), cols=1, imgsize=(224, 224), fontsize=12, fontfamily='monospace')</span>
</code></dt>
<dd>
<section class="desc"><p>Methods and parameters to
- display the results of classifying a list of images
- compare results
- calculate and display agreement between models</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>classifier</code></strong> :&ensp;<a title="pred_help.Classifier" href="#pred_help.Classifier"><code>Classifier</code></a></dt>
<dd>The Classifier object containing the results.</dd>
<dt><strong><code>pred2show</code></strong> :&ensp;<code>int</code></dt>
<dd>How many predictions to display</dd>
<dt><strong><code>fontsize</code></strong> :&ensp;<code>int</code></dt>
<dd>The fontsize</dd>
<dt><strong><code>fontfamily</code></strong> :&ensp;<code>str</code></dt>
<dd>The font family</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><a title="pred_help.Results" href="#pred_help.Results"><code>Results</code></a> <code>Object</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class Results:
  &#34;&#34;&#34;
  Methods and parameters to
  - display the results of classifying a list of images
  - compare results
  - calculate and display agreement between models
  &#34;&#34;&#34;
  def __init__(self, classifier:Classifier, pred2show=2, figsize=(3.0,3.5),
                     cols=1, imgsize=(224,224), fontsize=12, fontfamily=&#39;monospace&#39;):
    &#34;&#34;&#34;

    Args:
      classifier (Classifier): The Classifier object containing the results.
      pred2show (int): How many predictions to display
      fontsize (int): The fontsize
      fontfamily (str): The font family

    Returns:
      Results Object

    &#34;&#34;&#34;
    super()
    self.classifier = classifier
    self.resize_method = classifier.resize_method
    self.model_list = classifier.model_list
    self.model_dict = classifier.model_dict
    self.results    = classifier.results
    self.results_len = len(self.results)
    self.num_imgs    = classifier.num_imgs
    self.top_classes = classifier.top_classes
    self.top_probs  = classifier.top_probs
    self.fontsize   = fontsize
    self.fontfamily = fontfamily
    self.figsize    = figsize
    self.imgsize    = imgsize
    self.cols       = cols
    self.pred2show  = pred2show
    self.m2i = classifier.m2i
    self.i2m = classifier.i2m
    self.x   = 0
    self.y   = 0
    #
    self.ax           = None
    self.model_id     = None
    self.agree        = None
    self.agree_counts = None
    self.agree_diff   = None
    #
    self._init_agreement()



  def _init_agreement(self):

    cf  = self.classifier
    tc  = cf.top_classes
    tp  = cf.top_probs
    CML = cf.m2i(&#39;cml&#39;)

    # Allocate the 2 and 3 dim arrays we will need
    nm  = cf.num_models
    ni  = cf.num_imgs
    self.agree        = np.empty((nm, nm, ni), dtype=bool)
    self.agree_counts = np.empty((nm, nm), dtype=int)
    self.agree_diff   = np.empty((nm, nm, ni), dtype=float)

    # Populate the agreement tensors (CML will need to be revised ... see below)
    for im, m in enumerate(cf.model_list):
      for ik, k in enumerate(cf.model_list):
        self.agree[im, ik]        = tc[im, :, 0] == tc[ik, :, 0]
        self.agree_counts[im, ik] = self.agree[im, ik].sum()

    # Get accurate CML agreement counts
    for ir, r in enumerate(self.results):
      cml_comp = self._cml_compare(r)
      self.agree[CML, :, ir] = cml_comp
      self.agree[:, CML, ir] = cml_comp

    # Replace CML in the `agree` and `agree_counts` with accurate results
    for im, m in enumerate(cf.model_list):
      cml_sum = self.agree[CML, im, :].sum()
      self.agree_counts[CML, im] = cml_sum
      self.agree_counts[im, CML] = cml_sum

    # Populate the agreement difference matrix
    # If the two models agree on the top class, use the difference in probabilities,
    # if not, use 1.0 (they disagree 100% = no agreement )
    for im, m in enumerate(cf.model_list):
      for ik, k in enumerate(cf.model_list):
        for ir, r in enumerate(self.results):
          self.agree_diff[im, ik, ir]   = abs(tp[im, ir , 0] - tp[ik, ir , 0 ]) if self.agree[im,ik,ir] else 1.0


  def _cml_compare(self, res_item: dict) -&gt; ndarray:
    &#34;&#34;&#34;
    A results comparison function just for cml ...

    For this result item (i.e. image), compare the top CML label to the top label for the other models
    Return boolean array indicating which models agree with CML and which do not
    &#34;&#34;&#34;
    # Allocate an empty array, get the CML label, clean it up
    cml_agree = np.empty(len(self.model_list), dtype=bool)
    cml_label = res_item[&#39;cml&#39;].topL[0]
    #print(&#34;&#34;)
    #print(f&#34;cml label   = {cml_label}&#34;)
    cml_label.strip(&#39; ,-:;&#39;)
    #print(f&#34;cml label s = {cml_label}&#34;)
    # Compare the CML label to each of the top labels for the other models
    for im, m in enumerate(self.model_list):
      topL0 = res_item[m].topL[0]
      mitem = re.search(cml_label, topL0)
      cml_agree[im] = (mitem is not None)
      #print(f&#34;m     = {m}&#34;)
      #print(f&#34;topL0 = {topL0}&#34;)
      #print(f&#34;mitem = {mitem}&#34;)
      #print(f&#34;im    = {im}&#34;)
      #print(f&#34;cml_agree[im] = { cml_agree[im] }&#34;)
      #print(&#34;&#34;)
    #
    #print(f&#34;cml_agree = {cml_agree}&#34;)
    return cml_agree

  def agree_matrix(self):
    &#34;&#34;&#34;Show a heat-mapped agreement matrix&#34;&#34;&#34;
    fig, ax = plt.subplots(figsize=(8, 8))
    am, _   = heatmap(self.agree_counts, self.model_list, self.model_list,
                      ax=ax, cmap=&#34;PiYG&#34;, cbarlabel=&#34;Agreement&#34;)
    annotate_heatmap(am, valfmt=&#34;{x:d}&#34;, textcolors=[&#34;white&#34;, &#34;black&#34;], size=12)
    am = ax.imshow(self.agree_counts)
    return am

  def best_worst( self, model1:str, model2:str )-&gt;(int,int):
    &#34;&#34;&#34;
    **Agreement** - Returns indexes to the results with the best(= min diff) and worst(= max diff)
    agreement between two models

    Args:
      model1 (str): model id specified in model_params( e.g. &#34;onnx&#34;)
      model2 (str): model id specified in in model_params

    &#34;&#34;&#34;
    M1, M2 = self.m2i(model1), self.m2i(model2)
    # Copy the result array bkz we are going to zap it
    mmd = self.agree_diff[M1,M2].copy()
    best = mmd.argmin()
    # Zero all the &#34;1.0&#34; that represent the diff for non-matching classes
    mmd[mmd == 1.0] = 0.0
    # Now we can get an argmax diff for those classes that do match
    worst  = mmd.argmax()
    return best, worst


  def most_least(self)-&gt;dict:
    &#34;&#34;&#34;**Certainty** - Return the most and least certain results for all models&#34;&#34;&#34;
    tp = self.classifier.top_probs
    model_most_least  = [ [tp[im,:,0].argmax(), tp[im,:,0].argmin()] for im,m in enumerate(self.classifier.model_list) ]
    return model_most_least


  def _add_pred(self, pred:ImagePrediction, model_id:str=None, n2show=2,
                x:int=None, y:int=None):
    &#34;&#34;&#34;
    Add a Prediction to an existing axes.

    Args:
      pred (ImagePrediction): Image Prediction named tuple
      model_id (str): Model short name
      n2show (int): How many predictions to display
      x (int): starting x position for the text
      y (int): starting y position for the text

    Returns:
      Current value of x,y coordinates (x,y is also saved in Results object)
    &#34;&#34;&#34;

    ax = self.ax
    x  = if_None(x,self.x)
    y  = if_None(y,self.y)
    model  = if_None(model_id, self.model_id)
    fontsize = self.fontsize

    name_indent      = 10
    y_per_line  = int(1.9 * fontsize)+2
    y_between   = fontsize // 3
    results_indent  = name_indent + int(4* fontsize)

    # Show the model short name
    y +=  y_between
    ax.text(x + name_indent, y, model, verticalalignment=&#39;top&#39;,
            fontsize=self.fontsize, fontfamily=self.fontfamily)

    # Show the prediction results
    ax.text(x + results_indent, y, _fmt_results(pred, n2show=n2show),
             verticalalignment=&#39;top&#39;, fontsize=self.fontsize, fontfamily=self.fontfamily)
    self.x = x
    self.y = y + n2show * y_per_line
    return x, y

  def show_agreement(self,model1:str):
    &#34;&#34;&#34;Show agreement counts between `model1` and the others&#34;&#34;&#34;
    cf    = self.classifier
    M1    = cf.m2i(model1)
    nimgs = cf.num_imgs
    for im, m in enumerate(cf.model_list):
      agreed = self.agree_counts[M1, im]
      print(f&#34;{model1:7} and {m:7} agree on {agreed:4} of {nimgs:4} or {agreed / nimgs:2.2%}&#34;)


  def show_one(self, result:dict, models:list=None,
               pred2show:int = None, img_size=None, figsize=None, fontsize=None, fontfamily=None):
    &#34;&#34;&#34;
    Show selected or all predictions for one image ( = one result list item )

    Args:
      result (dict): The predictions for each model.
      models (list): For display, overrides the list of model names kept in Classifier.
      pred2show (int): How many of the top results to show for each prediction.

    Returns:
      ax : Object from plt.subplot call.
      None if there are no predictions

    &#34;&#34;&#34;
    cf          = self.classifier
    models      = if_None(models,   cf.model_list)
    figsize     = if_None(figsize,  self.figsize)
    img_size    = if_None(img_size, self.imgsize)
    fontsize    = if_None(fontsize, self.fontsize)
    fontfamily  = if_None(fontfamily, self.fontfamily)
    pred2show   = if_None(pred2show, self.pred2show)

    img_path    = Path(result[&#39;path&#39;])

    y_start = 0
    y_per_line = int(1.9 * fontsize)+2
    indent = 20

    # Show the image without frame or ticks
    fimg = ImageOps.fit(Image.open(img_path), size=img_size, method=cf.resize_method, centering=(0.5, 0.4))
    fig, ax = plt.subplots(1, 1, figsize=figsize, subplot_kw=dict(frame_on=False, xticks=[], yticks=[]))
    self.ax = ax
    ax.imshow(fimg)

    # Show the image file name
    x = img_size[0] + indent - 4
    y = y_start
    ax.text(x, y, img_path.name, fontsize=fontsize, fontfamily=fontfamily, verticalalignment=&#39;top&#39;)

    # Show the model(s) and their prediction probabilities
    self.x = x + indent
    self.y = y + y_per_line
    for m in models:
      self._add_pred(result[m], model_id=m, n2show=pred2show)

    plt.show()
    return ax

  def show(self, items:Union[int,list,tuple], models=None ):
    &#34;&#34;&#34;
    Show items from the result list
    Args:
      items (list): List of indexes into the results list.
        Or an int to show one item only.
      models (list): Constrains which models to show results for.

    &#34;&#34;&#34;
    results = self.classifier.results
    rlen    = self.results_len

    if type(items) is int:
      if items &lt;= rlen :
        self.show_one(results[items], models=models)
      return

    if type(items) is not list and type(items) is not tuple:
      raise TypeError(f&#34;type(items)={type(items)}; &#39;items&#39; must be an int or a list of ints&#34;)

    for n in items :
      if n &lt;= rlen :
        self.show_one(results[n], models=models)

  # def show_result(self, result:dict, pred2show:int=3, figsize=(3.0,3.5),
  #                 img_size=(224,224), fontsize=12, fontfamily=&#39;monospaced&#39;) :
  #   &#34;&#34;&#34;
  #   Show selected or all predictions for one image ( = one result item )
  #     Args:
  #       result(dict): The path to the image.
  #       img(Image): Image to use (optional, if not passed, image is read from &#39;img_path&#39;)
  #       pred2show(int): How many of the top results to show for each prediction.
  #         Set to True to display immediately after show_pred call
  #         Set to False to allow additional &#34;_add_pred&#34; calls before displaying.
  #         Use `plt.show()` to display when complete
  #
  #     Returns:
  #        axs: object from plt.subplot call
  #        x:   x position
  #        y:   y position
  #        None if there are no predictions
  #   &#34;&#34;&#34;
  #
  #   models = [m for m in result.keys() if m not in [&#39;name&#39;,&#39;path&#39;]]
  #
  #   img_path = Path(result[&#39;path&#39;])
  #
  #   y_start       = 4
  #   y_per_line    = int(1.9 * self.fontsize)
  #   indent        = 20
  #
  #   # Show the image  without frame or ticks
  #   fimg     = ImageOps.fit(Image.open(img_path), size=img_size, method=Image.NEAREST, centering=(0.5, 0.4))
  #   fig, ax  = plt.subplots(1, 1, figsize=figsize, subplot_kw=dict(frame_on=False, xticks=[], yticks=[]))
  #   self.ax  = ax
  #   ax.imshow(fimg)
  #
  #   # Show the image file name
  #   x = img_size[0] + indent - 4
  #   y = y_start
  #   ax.text(x, y, img_path.name, fontsize=fontsize, fontfamily=fontsize)
  #
  #   # Show the model(s) and their prediction probabilities
  #   self.x = x + indent
  #   self.y = y + y_per_line + 2
  #   for m in models[1:len(models)]:
  #      self._add_pred( result[m], model_id=m, n2show=pred2show)
  #
  #   plt.show()
  #   return ax

  def show_random(self,count=5,models=None) :
    import random
    display_list = random.sample(range(self.results_len), count)
    display_list.sort()
    print(f&#34;\nShowing results {display_list} \n  and top {self.pred2show} probabilities for each model&#34;)
    self.show(display_list, models=models)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="pred_help.Results.agree_matrix"><code class="name flex">
<span>def <span class="ident">agree_matrix</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Show a heat-mapped agreement matrix</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def agree_matrix(self):
  &#34;&#34;&#34;Show a heat-mapped agreement matrix&#34;&#34;&#34;
  fig, ax = plt.subplots(figsize=(8, 8))
  am, _   = heatmap(self.agree_counts, self.model_list, self.model_list,
                    ax=ax, cmap=&#34;PiYG&#34;, cbarlabel=&#34;Agreement&#34;)
  annotate_heatmap(am, valfmt=&#34;{x:d}&#34;, textcolors=[&#34;white&#34;, &#34;black&#34;], size=12)
  am = ax.imshow(self.agree_counts)
  return am</code></pre>
</details>
</dd>
<dt id="pred_help.Results.best_worst"><code class="name flex">
<span>def <span class="ident">best_worst</span></span>(<span>self, model1, model2)</span>
</code></dt>
<dd>
<section class="desc"><p><strong>Agreement</strong> - Returns indexes to the results with the best(= min diff) and worst(= max diff)
agreement between two models</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model1</code></strong> :&ensp;<code>str</code></dt>
<dd>model id specified in model_params( e.g. "onnx")</dd>
<dt><strong><code>model2</code></strong> :&ensp;<code>str</code></dt>
<dd>model id specified in in model_params</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def best_worst( self, model1:str, model2:str )-&gt;(int,int):
  &#34;&#34;&#34;
  **Agreement** - Returns indexes to the results with the best(= min diff) and worst(= max diff)
  agreement between two models

  Args:
    model1 (str): model id specified in model_params( e.g. &#34;onnx&#34;)
    model2 (str): model id specified in in model_params

  &#34;&#34;&#34;
  M1, M2 = self.m2i(model1), self.m2i(model2)
  # Copy the result array bkz we are going to zap it
  mmd = self.agree_diff[M1,M2].copy()
  best = mmd.argmin()
  # Zero all the &#34;1.0&#34; that represent the diff for non-matching classes
  mmd[mmd == 1.0] = 0.0
  # Now we can get an argmax diff for those classes that do match
  worst  = mmd.argmax()
  return best, worst</code></pre>
</details>
</dd>
<dt id="pred_help.Results.most_least"><code class="name flex">
<span>def <span class="ident">most_least</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p><strong>Certainty</strong> - Return the most and least certain results for all models</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def most_least(self)-&gt;dict:
  &#34;&#34;&#34;**Certainty** - Return the most and least certain results for all models&#34;&#34;&#34;
  tp = self.classifier.top_probs
  model_most_least  = [ [tp[im,:,0].argmax(), tp[im,:,0].argmin()] for im,m in enumerate(self.classifier.model_list) ]
  return model_most_least</code></pre>
</details>
</dd>
<dt id="pred_help.Results.show"><code class="name flex">
<span>def <span class="ident">show</span></span>(<span>self, items, models=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Show items from the result list</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>items</code></strong> :&ensp;<code>list</code></dt>
<dd>List of indexes into the results list.
Or an int to show one item only.</dd>
<dt><strong><code>models</code></strong> :&ensp;<code>list</code></dt>
<dd>Constrains which models to show results for.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def show(self, items:Union[int,list,tuple], models=None ):
  &#34;&#34;&#34;
  Show items from the result list
  Args:
    items (list): List of indexes into the results list.
      Or an int to show one item only.
    models (list): Constrains which models to show results for.

  &#34;&#34;&#34;
  results = self.classifier.results
  rlen    = self.results_len

  if type(items) is int:
    if items &lt;= rlen :
      self.show_one(results[items], models=models)
    return

  if type(items) is not list and type(items) is not tuple:
    raise TypeError(f&#34;type(items)={type(items)}; &#39;items&#39; must be an int or a list of ints&#34;)

  for n in items :
    if n &lt;= rlen :
      self.show_one(results[n], models=models)</code></pre>
</details>
</dd>
<dt id="pred_help.Results.show_agreement"><code class="name flex">
<span>def <span class="ident">show_agreement</span></span>(<span>self, model1)</span>
</code></dt>
<dd>
<section class="desc"><p>Show agreement counts between <code>model1</code> and the others</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def show_agreement(self,model1:str):
  &#34;&#34;&#34;Show agreement counts between `model1` and the others&#34;&#34;&#34;
  cf    = self.classifier
  M1    = cf.m2i(model1)
  nimgs = cf.num_imgs
  for im, m in enumerate(cf.model_list):
    agreed = self.agree_counts[M1, im]
    print(f&#34;{model1:7} and {m:7} agree on {agreed:4} of {nimgs:4} or {agreed / nimgs:2.2%}&#34;)</code></pre>
</details>
</dd>
<dt id="pred_help.Results.show_one"><code class="name flex">
<span>def <span class="ident">show_one</span></span>(<span>self, result, models=None, pred2show=None, img_size=None, figsize=None, fontsize=None, fontfamily=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Show selected or all predictions for one image ( = one result list item )</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>result</code></strong> :&ensp;<code>dict</code></dt>
<dd>The predictions for each model.</dd>
<dt><strong><code>models</code></strong> :&ensp;<code>list</code></dt>
<dd>For display, overrides the list of model names kept in Classifier.</dd>
<dt><strong><code>pred2show</code></strong> :&ensp;<code>int</code></dt>
<dd>How many of the top results to show for each prediction.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt>ax : Object from plt.subplot call.</dt>
<dt><code>None</code> <code>if</code> <code>there</code> <code>are</code> <code>no</code> <code>predictions</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def show_one(self, result:dict, models:list=None,
             pred2show:int = None, img_size=None, figsize=None, fontsize=None, fontfamily=None):
  &#34;&#34;&#34;
  Show selected or all predictions for one image ( = one result list item )

  Args:
    result (dict): The predictions for each model.
    models (list): For display, overrides the list of model names kept in Classifier.
    pred2show (int): How many of the top results to show for each prediction.

  Returns:
    ax : Object from plt.subplot call.
    None if there are no predictions

  &#34;&#34;&#34;
  cf          = self.classifier
  models      = if_None(models,   cf.model_list)
  figsize     = if_None(figsize,  self.figsize)
  img_size    = if_None(img_size, self.imgsize)
  fontsize    = if_None(fontsize, self.fontsize)
  fontfamily  = if_None(fontfamily, self.fontfamily)
  pred2show   = if_None(pred2show, self.pred2show)

  img_path    = Path(result[&#39;path&#39;])

  y_start = 0
  y_per_line = int(1.9 * fontsize)+2
  indent = 20

  # Show the image without frame or ticks
  fimg = ImageOps.fit(Image.open(img_path), size=img_size, method=cf.resize_method, centering=(0.5, 0.4))
  fig, ax = plt.subplots(1, 1, figsize=figsize, subplot_kw=dict(frame_on=False, xticks=[], yticks=[]))
  self.ax = ax
  ax.imshow(fimg)

  # Show the image file name
  x = img_size[0] + indent - 4
  y = y_start
  ax.text(x, y, img_path.name, fontsize=fontsize, fontfamily=fontfamily, verticalalignment=&#39;top&#39;)

  # Show the model(s) and their prediction probabilities
  self.x = x + indent
  self.y = y + y_per_line
  for m in models:
    self._add_pred(result[m], model_id=m, n2show=pred2show)

  plt.show()
  return ax</code></pre>
</details>
</dd>
<dt id="pred_help.Results.show_random"><code class="name flex">
<span>def <span class="ident">show_random</span></span>(<span>self, count=5, models=None)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def show_random(self,count=5,models=None) :
  import random
  display_list = random.sample(range(self.results_len), count)
  display_list.sort()
  print(f&#34;\nShowing results {display_list} \n  and top {self.pred2show} probabilities for each model&#34;)
  self.show(display_list, models=models)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-variables">Global variables</a></h3>
<ul class="">
<li><code><a title="pred_help.cifar" href="#pred_help.cifar">cifar</a></code></li>
<li><code><a title="pred_help.imagenet" href="#pred_help.imagenet">imagenet</a></code></li>
<li><code><a title="pred_help.mnist" href="#pred_help.mnist">mnist</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="pred_help.annotate_heatmap" href="#pred_help.annotate_heatmap">annotate_heatmap</a></code></li>
<li><code><a title="pred_help.heatmap" href="#pred_help.heatmap">heatmap</a></code></li>
<li><code><a title="pred_help.if_None" href="#pred_help.if_None">if_None</a></code></li>
<li><code><a title="pred_help.norm_for_imagenet" href="#pred_help.norm_for_imagenet">norm_for_imagenet</a></code></li>
<li><code><a title="pred_help.pred_for_coreml" href="#pred_help.pred_for_coreml">pred_for_coreml</a></code></li>
<li><code><a title="pred_help.pred_for_o2c" href="#pred_help.pred_for_o2c">pred_for_o2c</a></code></li>
<li><code><a title="pred_help.pred_for_onnx" href="#pred_help.pred_for_onnx">pred_for_onnx</a></code></li>
<li><code><a title="pred_help.pred_for_torch" href="#pred_help.pred_for_torch">pred_for_torch</a></code></li>
<li><code><a title="pred_help.show_pred" href="#pred_help.show_pred">show_pred</a></code></li>
<li><code><a title="pred_help.softmax" href="#pred_help.softmax">softmax</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="pred_help.Classifier" href="#pred_help.Classifier">Classifier</a></code></h4>
<ul class="">
<li><code><a title="pred_help.Classifier.classify" href="#pred_help.Classifier.classify">classify</a></code></li>
<li><code><a title="pred_help.Classifier.i2m" href="#pred_help.Classifier.i2m">i2m</a></code></li>
<li><code><a title="pred_help.Classifier.m2i" href="#pred_help.Classifier.m2i">m2i</a></code></li>
<li><code><a title="pred_help.Classifier.preds_for" href="#pred_help.Classifier.preds_for">preds_for</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pred_help.ImagePrediction" href="#pred_help.ImagePrediction">ImagePrediction</a></code></h4>
<ul class="">
<li><code><a title="pred_help.ImagePrediction.topI" href="#pred_help.ImagePrediction.topI">topI</a></code></li>
<li><code><a title="pred_help.ImagePrediction.topL" href="#pred_help.ImagePrediction.topL">topL</a></code></li>
<li><code><a title="pred_help.ImagePrediction.topP" href="#pred_help.ImagePrediction.topP">topP</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pred_help.ImageRepo" href="#pred_help.ImageRepo">ImageRepo</a></code></h4>
<ul class="">
<li><code><a title="pred_help.ImageRepo.labels_url" href="#pred_help.ImageRepo.labels_url">labels_url</a></code></li>
<li><code><a title="pred_help.ImageRepo.mean" href="#pred_help.ImageRepo.mean">mean</a></code></li>
<li><code><a title="pred_help.ImageRepo.std" href="#pred_help.ImageRepo.std">std</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pred_help.PredParams" href="#pred_help.PredParams">PredParams</a></code></h4>
<ul class="">
<li><code><a title="pred_help.PredParams.func" href="#pred_help.PredParams.func">func</a></code></li>
<li><code><a title="pred_help.PredParams.imgsize" href="#pred_help.PredParams.imgsize">imgsize</a></code></li>
<li><code><a title="pred_help.PredParams.labels" href="#pred_help.PredParams.labels">labels</a></code></li>
<li><code><a title="pred_help.PredParams.runtime" href="#pred_help.PredParams.runtime">runtime</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pred_help.Results" href="#pred_help.Results">Results</a></code></h4>
<ul class="two-column">
<li><code><a title="pred_help.Results.agree_matrix" href="#pred_help.Results.agree_matrix">agree_matrix</a></code></li>
<li><code><a title="pred_help.Results.best_worst" href="#pred_help.Results.best_worst">best_worst</a></code></li>
<li><code><a title="pred_help.Results.most_least" href="#pred_help.Results.most_least">most_least</a></code></li>
<li><code><a title="pred_help.Results.show" href="#pred_help.Results.show">show</a></code></li>
<li><code><a title="pred_help.Results.show_agreement" href="#pred_help.Results.show_agreement">show_agreement</a></code></li>
<li><code><a title="pred_help.Results.show_one" href="#pred_help.Results.show_one">show_one</a></code></li>
<li><code><a title="pred_help.Results.show_random" href="#pred_help.Results.show_random">show_random</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.6.3</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>